training:
  lr: 5e-2
  batch_size: 256
  epochs: 20
  accum_steps: 3
  debug_mode: true
  eval_interval: 10
  train_ratio: 0.2
  verbose: true
  seed: 17
  hist_save_interval: 50
  verbose_print_interval: 5
  workers: 0
  start_epoch: 0
  is_deteriminstic: false
  track_model_stats: false
  ckpt_dir: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/checkpoints
  ckpt_path: null
  ckpt_save_interval: 200

commit:
  commit_message: debug-for loggers and evaluation at interval
  commit_file_path: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/3-tb_logs/debug_commit_log.csv

logging:
  experiment_log_dir: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/
  tb_log_dir: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/3-tb_logs/
  logger_dir: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/

scheduler:
  type: ReduceLROnPlateau
  mode: min
  factor: 0.2
  patience: 50
  warmup_steps: 0

model:
  arch: resnet18
  pretrained: false

evaluation:
  eval_enabled: true
  val_enabled: true

amp:
  amp_enabled: false
  loss_scale: 1024.0
  opt_level: O0

optimizer:
  name: SGD
  momentum: 0.9
  weight_decay: 5e-4
  betas: [0.9, 0.99]
  criterion: CrossEntropyLoss

early_stopping:
  min_loss_improvement: 1e-2
  patience: 100

data:
  path: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/cifar100_data
  dataset_name: cifar10
  use_dummy: true

distributed_training:
  distributed: true
  dist_url: tcp://192.168.18.48:12345
  dist_backend: hccl
  master_addr: 192.168.18.48
  master_port: 12345
  multiprocessing_distributed: true
  device_type: npu
  device_list: [0, 1, 2, 3, 4, 5, 6, 7]

log_csv_path: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/3-tb_logs/commit_log.csv

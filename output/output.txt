/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/src/main.py:32: UserWarning: Deterministic mode can slow down training.
  warnings.warn('Deterministic mode can slow down training.')
Loading model...
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
Model adjusted for CIFAR-10/100 dataset
Model resnet18 loaded
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
--------\\\\\\\\||||||||////////--------\\\\\\\\||||||||/Set device npu:7 for training.
Set device npu:4 for training.
Set device npu:6 for training.
Set device npu:2 for training.
Set device npu:5 for training.
Set device npu:3 for training.
Set device npu:1 for training.
Set device npu:0 for training.
All layers are unfrozen for training
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
combine_grad           : None
combine_ddp            : None
ddp_replica_count      : 4
check_combined_tensors : None
user_cast_preferred    : None
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : 1024.0
combine_grad           : None
combine_ddp            : None
ddp_replica_count      : 4
check_combined_tensors : None
user_cast_preferred    : None
TensorBoard enabled at /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/tb_logs/
Epoch:[0][1 / 7]	loss: 4.9268e+00 (平均: 4.9268e+00)	top-1: 0.977 (平均: 0.977)	top-5 3.516 (平均: 3.516)	批次处理时间: 25.474秒 (平均: 25.474秒)	数据加载时间: 0.431秒 (平均: 0.431秒)


Epoch:[0] * loss: 4.895 top-1: 1.099 top-5 4.918 批次处理时间: 9.949 数据加载时间: 5.953
Test:[1 / 4]	loss: 8.1313e-01 (平均: 8.1313e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.739 (平均: 4.739)	批次处理时间: 1.172秒 (平均: 1.172秒)	数据加载时间: 5.273秒 (平均: 5.273秒)
Test: * loss: 0.505 top-1: 0.000 top-5 4.754 批次处理时间: 0.830 数据加载时间: 4.688
Epoch:[1][1 / 7]	loss: 4.9107e+00 (平均: 4.9107e+00)	top-1: 0.391 (平均: 0.391)	top-5 4.492 (平均: 4.492)	批次处理时间: 0.424秒 (平均: 0.424秒)	数据加载时间: 0.397秒 (平均: 0.397秒)


Epoch:[1] * loss: 4.884 top-1: 1.015 top-5 5.005 批次处理时间: 0.435 数据加载时间: 0.408
Test:[1 / 4]	loss: 4.2695e-01 (平均: 4.2695e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.729 (平均: 4.729)	批次处理时间: 1.562秒 (平均: 1.562秒)	数据加载时间: 7.031秒 (平均: 7.031秒)
Test: * loss: 0.424 top-1: 0.000 top-5 4.737 批次处理时间: 1.172 数据加载时间: 5.469
Epoch:[2][1 / 7]	loss: 4.8824e+00 (平均: 4.8824e+00)	top-1: 0.781 (平均: 0.781)	top-5 5.273 (平均: 5.273)	批次处理时间: 0.406秒 (平均: 0.406秒)	数据加载时间: 0.378秒 (平均: 0.378秒)


Epoch:[2] * loss: 4.870 top-1: 1.029 top-5 5.193 批次处理时间: 0.428 数据加载时间: 0.400
Test:[1 / 4]	loss: 4.2129e-01 (平均: 4.2129e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.713 (平均: 4.713)	批次处理时间: 1.758秒 (平均: 1.758秒)	数据加载时间: 5.469秒 (平均: 5.469秒)
Test: * loss: 0.429 top-1: 0.000 top-5 4.741 批次处理时间: 1.367 数据加载时间: 5.029
Epoch:[3][1 / 7]	loss: 4.8849e+00 (平均: 4.8849e+00)	top-1: 0.781 (平均: 0.781)	top-5 3.906 (平均: 3.906)	批次处理时间: 0.405秒 (平均: 0.405秒)	数据加载时间: 0.377秒 (平均: 0.377秒)


Epoch:[3] * loss: 4.857 top-1: 1.217 top-5 5.396 批次处理时间: 0.435 数据加载时间: 0.407
Test:[1 / 4]	loss: 4.3053e-01 (平均: 4.3053e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.722 (平均: 4.722)	批次处理时间: 1.562秒 (平均: 1.562秒)	数据加载时间: 5.859秒 (平均: 5.859秒)
Test: * loss: 0.424 top-1: 0.000 top-5 4.748 批次处理时间: 1.367 数据加载时间: 5.176
Epoch:[4][1 / 7]	loss: 4.8242e+00 (平均: 4.8242e+00)	top-1: 1.172 (平均: 1.172)	top-5 6.445 (平均: 6.445)	批次处理时间: 0.416秒 (平均: 0.416秒)	数据加载时间: 0.389秒 (平均: 0.389秒)


Epoch:[4] * loss: 4.834 top-1: 1.210 top-5 5.629 批次处理时间: 0.433 数据加载时间: 0.406
Test:[1 / 4]	loss: 4.1898e-01 (平均: 4.1898e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.736 (平均: 4.736)	批次处理时间: 2.344秒 (平均: 2.344秒)	数据加载时间: 7.617秒 (平均: 7.617秒)
Test: * loss: 0.420 top-1: 0.000 top-5 4.765 批次处理时间: 1.270 数据加载时间: 5.762
Epoch:[5][1 / 7]	loss: 4.8445e+00 (平均: 4.8445e+00)	top-1: 1.367 (平均: 1.367)	top-5 7.031 (平均: 7.031)	批次处理时间: 0.426秒 (平均: 0.426秒)	数据加载时间: 0.399秒 (平均: 0.399秒)


Epoch:[5] * loss: 4.828 top-1: 1.353 top-5 5.908 批次处理时间: 0.425 数据加载时间: 0.398
Test:[1 / 4]	loss: 4.1630e-01 (平均: 4.1630e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.769 (平均: 4.769)	批次处理时间: 2.539秒 (平均: 2.539秒)	数据加载时间: 6.250秒 (平均: 6.250秒)
Test: * loss: 0.427 top-1: 0.000 top-5 4.786 批次处理时间: 1.270 数据加载时间: 5.664
Epoch:[6][1 / 7]	loss: 4.8261e+00 (平均: 4.8261e+00)	top-1: 1.172 (平均: 1.172)	top-5 5.664 (平均: 5.664)	批次处理时间: 0.416秒 (平均: 0.416秒)	数据加载时间: 0.388秒 (平均: 0.388秒)


Epoch:[6] * loss: 4.822 top-1: 1.332 top-5 5.943 批次处理时间: 0.429 数据加载时间: 0.402
Test:[1 / 4]	loss: 4.1811e-01 (平均: 4.1811e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.763 (平均: 4.763)	批次处理时间: 1.172秒 (平均: 1.172秒)	数据加载时间: 6.836秒 (平均: 6.836秒)
Test: * loss: 0.416 top-1: 0.000 top-5 4.788 批次处理时间: 1.318 数据加载时间: 6.201
Epoch:[7][1 / 7]	loss: 4.8302e+00 (平均: 4.8302e+00)	top-1: 1.172 (平均: 1.172)	top-5 4.883 (平均: 4.883)	批次处理时间: 0.404秒 (平均: 0.404秒)	数据加载时间: 0.378秒 (平均: 0.378秒)


Epoch:[7] * loss: 4.800 top-1: 1.451 top-5 6.159 批次处理时间: 0.423 数据加载时间: 0.396
Test:[1 / 4]	loss: 4.2405e-01 (平均: 4.2405e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.784 (平均: 4.784)	批次处理时间: 1.172秒 (平均: 1.172秒)	数据加载时间: 5.273秒 (平均: 5.273秒)
Test: * loss: 0.423 top-1: 0.000 top-5 4.792 批次处理时间: 1.123 数据加载时间: 5.664
Epoch:[8][1 / 7]	loss: 4.8322e+00 (平均: 4.8322e+00)	top-1: 0.977 (平均: 0.977)	top-5 5.273 (平均: 5.273)	批次处理时间: 0.412秒 (平均: 0.412秒)	数据加载时间: 0.385秒 (平均: 0.385秒)


Epoch:[8] * loss: 4.791 top-1: 1.426 top-5 6.177 批次处理时间: 0.430 数据加载时间: 0.403
Test:[1 / 4]	loss: 4.1803e-01 (平均: 4.1803e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.749 (平均: 4.749)	批次处理时间: 0.977秒 (平均: 0.977秒)	数据加载时间: 6.641秒 (平均: 6.641秒)
Test: * loss: 0.416 top-1: 0.000 top-5 4.789 批次处理时间: 1.318 数据加载时间: 5.371
Epoch:[9][1 / 7]	loss: 4.7397e+00 (平均: 4.7397e+00)	top-1: 0.977 (平均: 0.977)	top-5 6.445 (平均: 6.445)	批次处理时间: 0.402秒 (平均: 0.402秒)	数据加载时间: 0.375秒 (平均: 0.375秒)


Epoch:[9] * loss: 4.787 top-1: 1.549 top-5 6.407 批次处理时间: 0.409 数据加载时间: 0.383
Test:[1 / 4]	loss: 4.0264e-01 (平均: 4.0264e-01)	top-1: 0.000 (平均: 0.000)	top-5 4.706 (平均: 4.706)	批次处理时间: 1.953秒 (平均: 1.953秒)	数据加载时间: 7.812秒 (平均: 7.812秒)
Test: * loss: 0.405 top-1: 0.000 top-5 4.752 批次处理时间: 1.465 数据加载时间: 6.738
Plotting Loss/train as train in Loss
tb_event has 10 items
Plotting Loss/val as val in Loss
tb_event has 10 items
Plotting Top1/train as train in Top1
tb_event has 10 items
Plotting Top1/val as val in Top1
tb_event has 10 items
Plotting Learning_Rate as Learning_Rate in Learning Rate
tb_event has 10 items
tb log exported successfully to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/tb_logs/figs/debug/resnet18-batch:4096-lr:5e-4-AdamW-exp1-metrics.png


--------------------
best acc1: 1.464844 at epoch 9


finished mp.spawn 
Total time cost：00h:02mins:46s

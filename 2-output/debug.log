`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
[2024-05-24 14:16:12,833] torch.distributed.run: [WARNING] 
[2024-05-24 14:16:12,833] torch.distributed.run: [WARNING] *****************************************
[2024-05-24 14:16:12,833] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-24 14:16:12,833] torch.distributed.run: [WARNING] *****************************************
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.

/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
Distributed Environment initialized with backend hccl.
Rank 0/2 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 1/2 reporting for duty.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Start running experiment 1/2 
with config: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/yamls/debug/debug-1.yaml 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Distributed environment initialized.
TensorBoard set up at /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/debug/arch-resnet18/2024-05-24||14-16-27/event
Experiment data logged successfully.
Yaml file copied to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/debug/arch-resnet18/2024-05-24||14-16-27/debug-1.yaml by ExperimentManager.
Experiment YAML copied successfully.
Starting new experiment folder. 
 
Logger directory updated to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/Exp1.
Seed set to 17. Training will be deterministic.
2024-05-24 14:16:27,063 - DEBUG - MainProcess - Main Logger initialized by MainManager.
2024-05-24 14:16:27,063 - INFO - MainProcess - Main logger is set up on node 0.
2024-05-24 14:16:27,064 - INFO - MainProcess - Distributed training initialized with backend: hccl, init_method: tcp://192.168.18.48:12345
Files already downloaded and verified
Starting new experiment folder. 
 
Logger directory updated to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/Exp1.
Files already downloaded and verified
Files already downloaded and verified
2024-05-24 14:16:43,369 - INFO - MainProcess - Dataset verified and ready at '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/cifar100_data'.
2024-05-24 14:16:43,369 - INFO - MainProcess - Dataset verified and downloaded successfully.
Initializing Worker: Global Rank = 1, Local Rank = 1, World Size = 2, ngpus_per_node = 2 

Initializing Worker: Global Rank = 0, Local Rank = 0, World Size = 2, ngpus_per_node = 2 

--Warning: Device do not support double dtype now, dtype cast repalce with float.
[W IscloseKernelNpu.cpp:32] Warning: Device do not support double dtype of rtol and atol now, dtype cast repalce with float. (function operator())
\\||//--\\2024-05-24 14:18:14,830 - INFO - MainProcess - 
 
Training Finished! 

2024-05-24 14:18:14,830 - INFO - MainProcess - Total time cost: 0h:1mins:29s
2024-05-24 14:18:14,830 - INFO - MainProcess - Best training accuracy: 10.0260, Best validation accuracy: 15.0000, Best epoch: 9.0, LR at best: 0.050000, Final test accuracy: 11.0000
2024-05-24 14:18:14,830 - INFO - MainProcess - lr: 5e-2, batch_size: 256, optimizer: SGD
Experiment metrics updated successfully.
All .log files copied from '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers' to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/Exp1'.
main finished! 



**************************************************
Experiment 1/2 completed!
**************************************************




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Start running experiment 2/2 
with config: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/yamls/debug/debug-2.yaml 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Distributed environment initialized.
Logger directory updated to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/Exp2.
TensorBoard set up at /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/debug/arch-resnet18/2024-05-24||14-18-14/event
Experiment data logged successfully.
Yaml file copied to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/debug/arch-resnet18/2024-05-24||14-18-14/debug-2.yaml by ExperimentManager.
Experiment YAML copied successfully.
Logger directory updated to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/Exp2.
Seed set to 17. Training will be deterministic.
2024-05-24 14:18:14,888 - DEBUG - MainProcess - Main Logger initialized by MainManager.
2024-05-24 14:18:14,888 - INFO - MainProcess - Main logger is set up on node 0.
2024-05-24 14:18:14,888 - INFO - MainProcess - Distributed training initialized with backend: hccl, init_method: tcp://192.168.18.48:12345
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
2024-05-24 14:18:31,038 - INFO - MainProcess - Dataset verified and ready at '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/cifar100_data'.
2024-05-24 14:18:31,038 - INFO - MainProcess - Dataset verified and downloaded successfully.
Initializing Worker: Global Rank = 1, Local Rank = 1, World Size = 2, ngpus_per_node = 2 

Initializing Worker: Global Rank = 0, Local Rank = 0, World Size = 2, ngpus_per_node = 2 

2024-05-24 14:19:18,568 - INFO - MainProcess - 
 
Training Finished! 

2024-05-24 14:19:18,569 - INFO - MainProcess - Total time cost: 0h:0mins:47s
2024-05-24 14:19:18,569 - INFO - MainProcess - Best training accuracy: 10.0260, Best validation accuracy: 15.0000, Best epoch: 9.0, LR at best: 0.050000, Final test accuracy: 13.0000
2024-05-24 14:19:18,569 - INFO - MainProcess - lr: 5e-2, batch_size: 256, optimizer: SGD
All YAML files have been processed. Exiting.
Cleaning up distributed process group...
Experiment metrics updated successfully.
All .log files copied from '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers' to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/debug/24-5-24/Exp2'.
main finished! 



**************************************************
Experiment 2/2 completed!
**************************************************


All YAML files have been processed. Exiting.
Cleaning up distributed process group...
Distributed process group cleaned up!
Distributed process group cleaned up!

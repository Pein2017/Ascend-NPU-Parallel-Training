`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
[2024-05-27 10:19:04,704] torch.distributed.run: [WARNING] 
[2024-05-27 10:19:04,704] torch.distributed.run: [WARNING] *****************************************
[2024-05-27 10:19:04,704] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-27 10:19:04,704] torch.distributed.run: [WARNING] *****************************************
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
Distributed Environment initialized with backend hccl.Distributed Environment initialized with backend hccl.

Rank 6/8 reporting for duty.Rank 3/8 reporting for duty.

Distributed Environment initialized with backend hccl.Distributed Environment initialized with backend hccl.

Rank 0/8 reporting for duty.Rank 1/8 reporting for duty.

Distributed Environment initialized with backend hccl.
Rank 5/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 4/8 reporting for duty.
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
Distributed Environment initialized with backend hccl.
Rank 7/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 2/8 reporting for duty.
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Start running experiment 1/6 
with config: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/yamls/letsgo/24-5-27/config1.yaml 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Distributed environment initialized.
TensorBoard set up at /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/lr-4/batch_size-8192/patience-30/2024-05-27||10-19-20/event
Experiment data logged successfully.
Yaml file copied to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/lr-4/batch_size-8192/patience-30/2024-05-27||10-19-20/config1.yaml by ExperimentManager.
Experiment YAML copied successfully.
Starting new experiment folder. 
 
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp1'
Seed set to 17. Training will be deterministic.
2024-05-27 10:19:20,821 - DEBUG - MainProcess - Main Logger initialized by MainManager.
2024-05-27 10:19:20,821 - INFO - MainProcess - Main logger is set up on node 0.
2024-05-27 10:19:20,822 - INFO - MainProcess - Distributed training initialized with backend: hccl, init_method: tcp://192.168.18.48:12345
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
2024-05-27 10:19:37,399 - INFO - MainProcess - Dataset verified and ready at '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/cifar100_data'.
2024-05-27 10:19:37,399 - INFO - MainProcess - Dataset verified and downloaded successfully.
Initializing Worker: Global Rank = 0, Local Rank = 0, World Size = 8, ngpus_per_node = 8 
Initializing Worker: Global Rank = 7, Local Rank = 7, World Size = 8, ngpus_per_node = 8 
Initializing Worker: Global Rank = 4, Local Rank = 4, World Size = 8, ngpus_per_node = 8 
Initializing Worker: Global Rank = 2, Local Rank = 2, World Size = 8, ngpus_per_node = 8 




Initializing Worker: Global Rank = 1, Local Rank = 1, World Size = 8, ngpus_per_node = 8 
Initializing Worker: Global Rank = 3, Local Rank = 3, World Size = 8, ngpus_per_node = 8 
Initializing Worker: Global Rank = 6, Local Rank = 6, World Size = 8, ngpus_per_node = 8 



Initializing Worker: Global Rank = 5, Local Rank = 5, World Size = 8, ngpus_per_node = 8 

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
--------\Warning: Device do not support double dtype now, dtype cast repalce with float.
[W IscloseKernelNpu.cpp:32] Warning: Device do not support double dtype of rtol and atol now, dtype cast repalce with float. (function operator())
\\\\\\\||||||||////////--------\\\\\\\\||||||||////////
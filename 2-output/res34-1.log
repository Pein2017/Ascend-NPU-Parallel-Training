`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
[2024-05-27 21:53:05,529] torch.distributed.run: [WARNING] 
[2024-05-27 21:53:05,529] torch.distributed.run: [WARNING] *****************************************
[2024-05-27 21:53:05,529] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-27 21:53:05,529] torch.distributed.run: [WARNING] *****************************************
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.


`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
Rank 1/8 reporting for duty.
Rank 5/8 reporting for duty.
Rank 2/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 0/8 reporting for duty.
Rank 4/8 reporting for duty.
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
Rank 3/8 reporting for duty.
Rank 6/8 reporting for duty.
Rank 7/8 reporting for duty.


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Start running experiment 1/2 
with config: /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/yamls/letsgo/24-5-27/config5.yaml 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


Distributed environment initialized.
TensorBoard set up at /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/lr-3e-1/batch_size-1024/2024-05-27||21-53-19/event
Experiment data logged successfully.
Yaml file copied to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/lr-3e-1/batch_size-1024/2024-05-27||21-53-19/config5.yaml by ExperimentManager.
Experiment YAML copied successfully.
Logger directory updated to '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/4-loggers/letsgo/24-5-27/Exp5'
Seed set to 17. Training will be deterministic.
2024-05-27 21:53:35,711 - INFO - MainProcess - Dataset verified and downloaded successfully.
Initializing Worker: Global Rank = 0, Local Rank = 0, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 3, Local Rank = 3, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 4, Local Rank = 4, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 7, Local Rank = 7, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 6, Local Rank = 6, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 5, Local Rank = 5, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 1, Local Rank = 1, World Size = 8, ngpus_per_node = 8 

Initializing Worker: Global Rank = 2, Local Rank = 2, World Size = 8, ngpus_per_node = 8 

--------\Warning: Device do not support double dtype now, dtype cast repalce with float.
[W IscloseKernelNpu.cpp:32] Warning: Device do not support double dtype of rtol and atol now, dtype cast repalce with float. (function operator())
\\\\\\\||||||||////////--------\\\\\\\\||||||||
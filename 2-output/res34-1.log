`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
[2024-05-16 19:38:29,574] torch.distributed.run: [WARNING] 
[2024-05-16 19:38:29,574] torch.distributed.run: [WARNING] *****************************************
[2024-05-16 19:38:29,574] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-05-16 19:38:29,574] torch.distributed.run: [WARNING] *****************************************
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/dynamo/__init__.py:18: UserWarning: Register eager implementation for the 'npu' backend of dynamo, as torch_npu was not compiled with torchair.
  warnings.warn(
Distributed Environment initialized with backend hccl.Distributed Environment initialized with backend hccl.

Rank 3/8 reporting for duty.Rank 2/8 reporting for duty.

Distributed Environment initialized with backend hccl.
Rank 6/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 1/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 0/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 4/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 5/8 reporting for duty.
Distributed Environment initialized with backend hccl.
Rank 7/8 reporting for duty.
Distributed environment initialized.
Distributed environment initialized.
Distributed environment initialized.
Distributed environment initialized.
TensorBoard set up at /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/lr-5e-4/2024-05-16_19-38-44/event
Creating new log csv file at "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/3-tb_logs/commit_log.csv"...
Experiment data logged successfully.
Experiment data logged successfully.
File copied to /data/Pein/Pytorch/Ascend-NPU-Parallel-Training/5-experiment_logs/lr-5e-4/2024-05-16_19-38-44/config_1.yaml
Experiment YAML copied successfully.
Seed set to 17. Training will be deterministic.
2024-05-16 19:38:44,975 - DEBUG - MainProcess - Main Logger initialized.
2024-05-16 19:38:44,975 - INFO - MainProcess - Main logger is set up on node 0.
2024-05-16 19:38:44,975 - INFO - MainProcess - Distributed training initialized with backend: hccl, init_method: tcp://192.168.18.48:12345
Distributed environment initialized.
Distributed environment initialized.
Distributed environment initialized.
Distributed environment initialized.
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
2024-05-16 19:39:01,397 - INFO - MainProcess - Dataset verified and ready at '/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/cifar100_data'.
2024-05-16 19:39:01,398 - INFO - MainProcess - Dataset verified and downloaded successfully.
2024-05-16 19:39:01,398 - INFO - MainProcess - Attempting to synchronize all processes...
Initializing Worker: Global Rank = 6, Local Rank = 6, World Size = 8, ngpus_per_node = 8
Initializing Worker: Global Rank = 4, Local Rank = 4, World Size = 8, ngpus_per_node = 8
Initializing Worker: Global Rank = 1, Local Rank = 1, World Size = 8, ngpus_per_node = 8
Initializing Worker: Global Rank = 5, Local Rank = 5, World Size = 8, ngpus_per_node = 82024-05-16 19:39:06,145 - INFO - MainProcess - All processes synchronized successfully.
Initializing Worker: Global Rank = 2, Local Rank = 2, World Size = 8, ngpus_per_node = 8

Initializing Worker: Global Rank = 0, Local Rank = 0, World Size = 8, ngpus_per_node = 8
Initializing Worker: Global Rank = 7, Local Rank = 7, World Size = 8, ngpus_per_node = 8
Initializing Worker: Global Rank = 3, Local Rank = 3, World Size = 8, ngpus_per_node = 8
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verifiedFiles already downloaded and verified

Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
--------\Warning: Device do not support double dtype now, dtype cast repalce with float.
[W IscloseKernelNpu.cpp:32] Warning: Device do not support double dtype of rtol and atol now, dtype cast repalce with float. (function operator())
\\\\\\\||||||||Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239304
Terminating process 236562
Terminating process 239298
Terminating process 239301
Terminating process 239302
Terminating process 239293
Terminating process 236550
Terminating process 236565
Terminating process 236555
Terminating process 236557
Terminating process 239295
Terminating process 239297
Terminating process 236552
Terminating process 236548
Terminating process 236635
Terminating process 239306
Terminating process 236560
Terminating process 236162
Received signal to terminate. Cleaning up...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239304
Terminating process 236562
Terminating process 239298
Terminating process 239301
Terminating process 239302
Terminating process 239293
Terminating process 236550
Terminating process 236565
Terminating process 236555
Terminating process 236557
Terminating process 239295
Terminating process 239297
Terminating process 236552
Terminating process 236548
Terminating process 236635
Terminating process 239306
Terminating process 236560
Terminating process 236162
Received signal to terminate. Cleaning up...
Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239377
Terminating process 237297
Terminating process 237302
Terminating process 239373
Terminating process 239375
Terminating process 237308
Terminating process 237313
Terminating process 237321
Terminating process 239371
Terminating process 239370
Terminating process 237293
Terminating process 239376
Terminating process 236799
Terminating process 239372
Terminating process 237318
Terminating process 239374
Terminating process 237287
Terminating process 237391
Received signal to terminate. Cleaning up...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239377
Terminating process 237297
Terminating process 237302
Terminating process 239373
Terminating process 239375
Terminating process 237308
Terminating process 237313
Terminating process 237321
Terminating process 239371
Terminating process 239370
Terminating process 237293
Terminating process 239376
Terminating process 236799
Terminating process 239372
Terminating process 237318
Received signal to terminate. Cleaning up...Terminating process 239374

Terminating process 237287
Terminating process 237391
Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 237221
Terminating process 239273
Terminating process 239271
Terminating process 237193
Terminating process 237188
Terminating process 237191
Terminating process 239267
Terminating process 239268
Terminating process 239270
Terminating process 236465
Terminating process 237189
Terminating process 239266
Terminating process 237186Received signal to terminate. Cleaning up...

Terminating process 239269
Terminating process 237192
Cleaning up distributed process group...
Terminating process 239272
Terminating process 237187
Terminating process 237182
Distributed process group cleaned up!
Terminating process 237221
Terminating process 239273
Terminating process 239271
Terminating process 237193
Terminating process 237188
Terminating process 237191
Terminating process 239267
Terminating process 239268
Terminating process 239270
Terminating process 236465
Terminating process 237189
Terminating process 239266
Received signal to terminate. Cleaning up...Terminating process 237186

Terminating process 239269
Terminating process 237192
Terminating process 239272
Terminating process 237187
Terminating process 237182
Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 237285
Terminating process 237277
Terminating process 239323
Terminating process 239325
Terminating process 237281
Terminating process 239329
Terminating process 237279
Terminating process 239324
Terminating process 239321
Terminating process 237275
Terminating process 239328
Terminating process 237283
Terminating process 239326
Terminating process 237276
Terminating process 236797
Terminating process 239322
Terminating process 237274
Received signal to terminate. Cleaning up...Terminating process 237364

Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 237285
Terminating process 237277
Terminating process 239323
Terminating process 239325
Terminating process 237281
Terminating process 239329
Terminating process 237279
Terminating process 239324
Terminating process 239321
Terminating process 237275
Terminating process 239328
Terminating process 237283
Terminating process 239326
Terminating process 237276
Terminating process 236797
Terminating process 239322
Terminating process 237274
Terminating process 237364
Received signal to terminate. Cleaning up...
Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 237278
Terminating process 237377
Terminating process 239363
Terminating process 237286
Terminating process 239366
Terminating process 239369
Terminating process 237295
Terminating process 239364
Terminating process 237291
Terminating process 237282
Terminating process 237298
Terminating process 239368
Terminating process 239362
Terminating process 237280
Terminating process 239365
Terminating process 236800
Terminating process 237284
Terminating process 239367
Received signal to terminate. Cleaning up...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 237278
Terminating process 237377
Terminating process 239363
Terminating process 237286
Terminating process 239366
Terminating process 239369
Terminating process 237295
Terminating process 239364
Terminating process 237291
Terminating process 237282
Terminating process 237298
Terminating process 239368
Terminating process 239362
Terminating process 237280
Terminating process 239365
Terminating process 236800
Terminating process 237284
Received signal to terminate. Cleaning up...Terminating process 239367

Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239354
Terminating process 239353
Terminating process 239348
Terminating process 237237
Terminating process 237243
Terminating process 237241
Terminating process 239351
Terminating process 237239
Terminating process 239352
Terminating process 239355
Terminating process 237240
Terminating process 236590
Terminating process 237238
Terminating process 237236
Terminating process 237272
Terminating process 239349
Received signal to terminate. Cleaning up...Terminating process 239350

Terminating process 237242
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239354
Terminating process 239353
Terminating process 239348
Terminating process 237237
Terminating process 237243
Terminating process 237241
Terminating process 239351
Terminating process 237239
Terminating process 239352
Terminating process 239355
Terminating process 237240
Terminating process 236590
Terminating process 237238
Received signal to terminate. Cleaning up...Terminating process 237236

Terminating process 237272
Terminating process 239349
Terminating process 239350
Terminating process 237242
/Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 236683
Terminating process 236670
Terminating process 236795
Terminating process 239294
Terminating process 239300
Terminating process 236707
Terminating process 239303
Terminating process 236687
Terminating process 239296
Terminating process 239305
Terminating process 236701
Terminating process 239299
Terminating process 236697
Terminating process 236704
Terminating process 236677
Terminating process 236243
Terminating process 239292
Terminating process 239307
Received signal to terminate. Cleaning up...
Cleaning up distributed process group...
Distributed process group cleaned up!Terminating process 236683

Terminating process 236670
Terminating process 236795
Terminating process 239294
Terminating process 239300
Terminating process 236707
Terminating process 239303
Terminating process 236687
Terminating process 239296
Terminating process 239305
Terminating process 236701
Terminating process 239299
Terminating process 236697
Terminating process 236704
Terminating process 236677
Terminating process 236243
Terminating process 239292
Received signal to terminate. Cleaning up...Terminating process 239307

Error during main:
Traceback (most recent call last):
  File "1-src/main_class.py", line 309, in <module>
    main(default_yaml_path, experiment_yaml_path)
  File "1-src/main_class.py", line 261, in main
    main_manager.run_training()
  File "1-src/main_class.py", line 172, in run_training
    self.start_worker_and_get_result()
  File "1-src/main_class.py", line 213, in start_worker_and_get_result
    worker.execute_main_task()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 458, in execute_main_task
    self.run_training()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/worker_class.py", line 513, in run_training
    self.trainer.train_multiple_epochs()
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 479, in train_multiple_epochs
    tracker, train_losses_meter, train_top1, train_top5 = self.train_one_epoch(
  File "/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/1-src/trainer.py", line 214, in train_one_epoch
    accumulated_loss.backward()
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [npuFloatType [512]] is at version 7; expected version 6 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Forcefully terminating the distributed training...
Cleaning up distributed process group...
Distributed process group cleaned up!
Terminating process 239318
Terminating process 236471
Terminating process 236468
Terminating process 239311
Terminating process 236466
Terminating process 236473
Terminating process 239315
Terminating process 236469
Terminating process 239316
Terminating process 236110
Terminating process 236464
Terminating process 236467
Terminating process 236529
Terminating process 239312
Terminating process 239313
Terminating process 239314
Received signal to terminate. Cleaning up...Terminating process 239317

Terminating process 236474
Cleaning up distributed process group...
Terminating process 239318
Terminating process 236471
Terminating process 236468
Terminating process 239311
Terminating process 236466
Terminating process 236473
Terminating process 239315
Terminating process 236469
Terminating process 239316
Terminating process 236110
Terminating process 236464
Terminating process 236467
Terminating process 236529Received signal to terminate. Cleaning up...

Terminating process 239312
Terminating process 239313
Terminating process 239314
Terminating process 239317
Terminating process 236474
Distributed process group cleaned up!
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
`TORCH_CPP_LOG_LEVEL` environment variable cannot be parsed. Valid values are `INFO`, `WARNING`, `ERROR`, and `FATAL` or their numerical equivalents `0`, `1`, `2`, and `3`.
[2024-05-16 19:40:04,736] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 235946) of binary: /root/miniconda3/envs/Pein38/bin/python
Traceback (most recent call last):
  File "/root/miniconda3/envs/Pein38/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/distributed/run.py", line 806, in main
    run(args)
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/distributed/run.py", line 797, in run
    elastic_launch(
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
1-src/main_class.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 235947)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 235948)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 235949)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[4]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 235950)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[5]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 235951)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[6]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 235952)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[7]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 235953)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-16_19:40:04
  host      : bms-sldzkf-01
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 235946)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

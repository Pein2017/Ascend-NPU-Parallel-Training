/data/Pein/Pytorch/Ascend-NPU-Parallel-Training/main_mine.py:35: UserWarning: Deterministic mode can slow down training.
  warnings.warn('Deterministic mode can slow down training.')
loading model...
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
Use GPU/NPU: 0 for training
Use GPU/NPU: 7 for training
Use GPU/NPU: 5 for training
Use GPU/NPU: 4 for training
Use GPU/NPU: 1 for training
Use GPU/NPU: 2 for training
Use GPU/NPU: 6 for training
Use GPU/NPU: 3 for training
--------set device to npu:5
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
set device to npu:7
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
set device to npu:4
set device to npu:6
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
set device to npu:3
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
set device to npu:0
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
combine_grad           : None
combine_ddp            : None
ddp_replica_count      : 4
check_combined_tensors : None
user_cast_preferred    : None
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : 1024.0
combine_grad           : None
combine_ddp            : None
ddp_replica_count      : 4
check_combined_tensors : None
user_cast_preferred    : None
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
set device to npu:2
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
set device to npu:1
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch_npu/utils/storage.py:50: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  tensor = torch.tensor([], dtype=storage.dtype, device=storage.device)
\\\\\\\\||||||||////////--------\\\\\\\\||||||||////////--------\\\\\\\\||||||||////////--------\\\\\\\\||||||||////////Epoch: [1][ 1/39]	Time 79.502 (79.502)	Data  7.987 ( 7.987)	损失 2.5417e+00 (2.5417e+00)	准确率@1  12.50 ( 12.50)	准确率@5  46.88 ( 46.88)
Epoch: [1][ 6/39]	Time  0.021 (26.044)	Data  0.002 (13.340)	损失 2.5914e+00 (2.6128e+00)	准确率@1   9.38 ( 10.42)	准确率@5  56.25 ( 48.44)
Epoch: [1][11/39]	Time  0.023 (14.216)	Data  0.003 ( 7.278)	损失 2.3254e+00 (2.5431e+00)	准确率@1  25.00 ( 12.50)	准确率@5  56.25 ( 55.11)
Epoch: [1][16/39]	Time  0.028 ( 9.782)	Data  0.006 ( 5.006)	损失 2.7496e+00 (2.5725e+00)	准确率@1   6.25 ( 10.74)	准确率@5  43.75 ( 55.27)
Epoch: [1][21/39]	Time  0.026 ( 7.459)	Data  0.003 ( 3.815)	损失 2.5601e+00 (2.5445e+00)	准确率@1  15.62 ( 11.90)	准确率@5  50.00 ( 55.95)
Epoch: [1][26/39]	Time  0.025 ( 6.029)	Data  0.003 ( 3.082)	损失 2.3782e+00 (2.5233e+00)	准确率@1  18.75 ( 12.74)	准确率@5  68.75 ( 57.33)
Epoch: [1][31/39]	Time  0.026 ( 5.061)	Data  0.004 ( 2.585)	损失 2.5077e+00 (2.5051e+00)	准确率@1   6.25 ( 12.60)	准确率@5  59.38 ( 58.17)
Epoch: [1][36/39]	Time  0.025 ( 4.362)	Data  0.003 ( 2.227)	损失 2.6410e+00 (2.4959e+00)	准确率@1   6.25 ( 12.67)	准确率@5  62.50 ( 58.51)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 80.324 (80.324)	Data  7.984 ( 7.984)	损失 2.6016e+00 (2.6016e+00)	准确率@1   3.12 (  3.12)	准确率@5  46.88 ( 46.88)
Epoch: [1][ 6/39]	Time  0.022 (26.044)	Data  0.002 (13.203)	损失 2.3720e+00 (2.5533e+00)	准确率@1  12.50 ( 13.54)	准确率@5  65.62 ( 54.17)
Epoch: [1][11/39]	Time  0.022 (14.216)	Data  0.002 ( 7.202)	损失 2.3859e+00 (2.5533e+00)	准确率@1  21.88 ( 13.64)	准确率@5  71.88 ( 55.11)
Epoch: [1][16/39]	Time  0.026 ( 9.781)	Data  0.002 ( 4.953)	损失 2.5253e+00 (2.5581e+00)	准确率@1   6.25 ( 11.52)	准确率@5  53.12 ( 54.10)
Epoch: [1][21/39]	Time  0.025 ( 7.459)	Data  0.002 ( 3.774)	损失 2.5281e+00 (2.5531e+00)	准确率@1   6.25 ( 11.31)	准确率@5  59.38 ( 54.61)
Epoch: [1][26/39]	Time  0.026 ( 6.029)	Data  0.002 ( 3.049)	损失 2.3818e+00 (2.5307e+00)	准确率@1  21.88 ( 12.38)	准确率@5  59.38 ( 55.65)
Epoch: [1][31/39]	Time  0.023 ( 5.061)	Data  0.002 ( 2.558)	损失 2.2755e+00 (2.5165e+00)	准确率@1  28.12 ( 13.41)	准确率@5  65.62 ( 56.35)
Epoch: [1][36/39]	Time  0.026 ( 4.362)	Data  0.002 ( 2.203)	损失 2.2840e+00 (2.4900e+00)	准确率@1  18.75 ( 14.15)	准确率@5  65.62 ( 57.38)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 79.682 (79.682)	Data  7.990 ( 7.990)	损失 2.3983e+00 (2.3983e+00)	准确率@1  15.62 ( 15.62)	准确率@5  65.62 ( 65.62)
Epoch: [1][ 6/39]	Time  0.021 (26.044)	Data  0.002 (13.310)	损失 2.6393e+00 (2.5815e+00)	准确率@1  15.62 ( 11.98)	准确率@5  53.12 ( 51.56)
Epoch: [1][11/39]	Time  0.023 (14.216)	Data  0.002 ( 7.262)	损失 2.9115e+00 (2.6522e+00)	准确率@1   6.25 ( 10.80)	准确率@5  25.00 ( 46.59)
Epoch: [1][16/39]	Time  0.026 ( 9.781)	Data  0.002 ( 4.993)	损失 2.3731e+00 (2.6373e+00)	准确率@1  18.75 ( 10.74)	准确率@5  50.00 ( 47.07)
Epoch: [1][21/39]	Time  0.026 ( 7.459)	Data  0.002 ( 3.805)	损失 2.4939e+00 (2.6197e+00)	准确率@1  12.50 ( 10.71)	准确率@5  53.12 ( 47.02)
Epoch: [1][26/39]	Time  0.026 ( 6.029)	Data  0.002 ( 3.074)	损失 2.5653e+00 (2.6003e+00)	准确率@1  18.75 ( 11.06)	准确率@5  53.12 ( 49.04)
Epoch: [1][31/39]	Time  0.025 ( 5.061)	Data  0.002 ( 2.579)	损失 2.3915e+00 (2.5752e+00)	准确率@1  12.50 ( 11.29)	准确率@5  50.00 ( 50.40)
Epoch: [1][36/39]	Time  0.025 ( 4.362)	Data  0.002 ( 2.221)	损失 2.4297e+00 (2.5467e+00)	准确率@1  15.62 ( 11.89)	准确率@5  53.12 ( 51.65)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 79.659 (79.659)	Data  8.002 ( 8.002)	损失 2.6112e+00 (2.6112e+00)	准确率@1   9.38 (  9.38)	准确率@5  50.00 ( 50.00)
Epoch: [1][ 6/39]	Time  0.021 (26.044)	Data  0.002 (13.316)	损失 2.4504e+00 (2.5205e+00)	准确率@1   9.38 ( 10.94)	准确率@5  62.50 ( 55.73)
Epoch: [1][11/39]	Time  0.023 (14.216)	Data  0.002 ( 7.265)	损失 2.4151e+00 (2.5212e+00)	准确率@1   6.25 ( 11.65)	准确率@5  62.50 ( 55.68)
Epoch: [1][16/39]	Time  0.025 ( 9.781)	Data  0.005 ( 4.996)	损失 2.7493e+00 (2.5532e+00)	准确率@1   9.38 ( 11.33)	准确率@5  40.62 ( 53.52)
Epoch: [1][21/39]	Time  0.026 ( 7.459)	Data  0.002 ( 3.807)	损失 2.4526e+00 (2.5621e+00)	准确率@1  12.50 ( 10.42)	准确率@5  56.25 ( 53.87)
Epoch: [1][26/39]	Time  0.030 ( 6.029)	Data  0.003 ( 3.076)	损失 2.3786e+00 (2.5516e+00)	准确率@1   6.25 ( 10.46)	准确率@5  65.62 ( 54.09)
Epoch: [1][31/39]	Time  0.025 ( 5.061)	Data  0.003 ( 2.580)	损失 2.6047e+00 (2.5337e+00)	准确率@1  12.50 ( 10.79)	准确率@5  56.25 ( 55.44)
Epoch: [1][36/39]	Time  0.025 ( 4.362)	Data  0.002 ( 2.222)	损失 2.5517e+00 (2.5202e+00)	准确率@1   6.25 ( 11.28)	准确率@5  53.12 ( 56.34)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 80.335 (80.335)	Data  7.924 ( 7.924)	损失 2.4985e+00 (2.4985e+00)	准确率@1   6.25 (  6.25)	准确率@5  62.50 ( 62.50)
Epoch: [1][ 6/39]	Time  0.021 (26.044)	Data  0.002 (13.191)	损失 2.5627e+00 (2.5172e+00)	准确率@1  15.62 ( 11.46)	准确率@5  59.38 ( 57.29)
Epoch: [1][11/39]	Time  0.023 (14.216)	Data  0.002 ( 7.196)	损失 2.5427e+00 (2.5289e+00)	准确率@1  12.50 ( 12.22)	准确率@5  59.38 ( 57.95)
Epoch: [1][16/39]	Time  0.026 ( 9.781)	Data  0.001 ( 4.948)	损失 2.4378e+00 (2.5353e+00)	准确率@1  15.62 ( 11.52)	准确率@5  59.38 ( 56.25)
Epoch: [1][21/39]	Time  0.026 ( 7.459)	Data  0.001 ( 3.771)	损失 2.4207e+00 (2.5164e+00)	准确率@1  15.62 ( 11.46)	准确率@5  62.50 ( 57.44)
Epoch: [1][26/39]	Time  0.026 ( 6.029)	Data  0.002 ( 3.046)	损失 2.2787e+00 (2.4898e+00)	准确率@1  18.75 ( 11.78)	准确率@5  68.75 ( 58.05)
Epoch: [1][31/39]	Time  0.026 ( 5.061)	Data  0.001 ( 2.555)	损失 2.3474e+00 (2.4852e+00)	准确率@1  18.75 ( 12.50)	准确率@5  62.50 ( 57.66)
Epoch: [1][36/39]	Time  0.025 ( 4.362)	Data  0.001 ( 2.201)	损失 2.4316e+00 (2.4770e+00)	准确率@1  12.50 ( 12.76)	准确率@5  53.12 ( 57.81)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 79.597 (79.597)	Data  7.896 ( 7.896)	损失 2.7066e+00 (2.7066e+00)	准确率@1  15.62 ( 15.62)	准确率@5  56.25 ( 56.25)
Epoch: [1][ 6/39]	Time  0.021 (26.043)	Data  0.002 (13.309)	损失 2.4426e+00 (2.6145e+00)	准确率@1   9.38 (  9.38)	准确率@5  53.12 ( 53.12)
Epoch: [1][11/39]	Time  0.023 (14.216)	Data  0.002 ( 7.261)	损失 2.6767e+00 (2.5879e+00)	准确率@1   0.00 (  9.09)	准确率@5  43.75 ( 53.98)
Epoch: [1][16/39]	Time  0.026 ( 9.781)	Data  0.002 ( 4.993)	损失 2.6053e+00 (2.5924e+00)	准确率@1  12.50 ( 10.35)	准确率@5  53.12 ( 54.49)
Epoch: [1][21/39]	Time  0.025 ( 7.459)	Data  0.002 ( 3.805)	损失 2.4089e+00 (2.5910e+00)	准确率@1  15.62 ( 10.71)	准确率@5  62.50 ( 54.46)
Epoch: [1][26/39]	Time  0.026 ( 6.029)	Data  0.002 ( 3.074)	损失 2.4487e+00 (2.5466e+00)	准确率@1   6.25 ( 11.78)	准确率@5  59.38 ( 55.17)
Epoch: [1][31/39]	Time  0.026 ( 5.061)	Data  0.002 ( 2.579)	损失 2.3129e+00 (2.5311e+00)	准确率@1  21.88 ( 11.79)	准确率@5  68.75 ( 55.95)
Epoch: [1][36/39]	Time  0.027 ( 4.362)	Data  0.002 ( 2.221)	损失 2.5314e+00 (2.5256e+00)	准确率@1  12.50 ( 11.89)	准确率@5  56.25 ( 56.86)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 79.709 (79.709)	Data  7.925 ( 7.925)	损失 2.5312e+00 (2.5312e+00)	准确率@1   9.38 (  9.38)	准确率@5  43.75 ( 43.75)
Epoch: [1][ 6/39]	Time  0.021 (26.043)	Data  0.002 (13.441)	损失 2.5411e+00 (2.5631e+00)	准确率@1   6.25 ( 10.94)	准确率@5  56.25 ( 52.60)
Epoch: [1][11/39]	Time  0.022 (14.216)	Data  0.002 ( 7.333)	损失 2.2452e+00 (2.5122e+00)	准确率@1  18.75 ( 11.08)	准确率@5  65.62 ( 55.11)
Epoch: [1][16/39]	Time  0.026 ( 9.781)	Data  0.002 ( 5.042)	损失 2.6191e+00 (2.5248e+00)	准确率@1   9.38 ( 10.55)	准确率@5  46.88 ( 53.91)
Epoch: [1][21/39]	Time  0.025 ( 7.459)	Data  0.002 ( 3.843)	损失 2.5248e+00 (2.5127e+00)	准确率@1  12.50 ( 10.86)	准确率@5  56.25 ( 54.61)
Epoch: [1][26/39]	Time  0.026 ( 6.029)	Data  0.002 ( 3.104)	损失 2.6361e+00 (2.4927e+00)	准确率@1  12.50 ( 11.66)	准确率@5  46.88 ( 55.89)
Epoch: [1][31/39]	Time  0.026 ( 5.061)	Data  0.002 ( 2.604)	损失 2.5850e+00 (2.4752e+00)	准确率@1  12.50 ( 11.69)	准确率@5  59.38 ( 56.85)
Epoch: [1][36/39]	Time  0.025 ( 4.362)	Data  0.002 ( 2.243)	损失 2.4665e+00 (2.4603e+00)	准确率@1   6.25 ( 12.07)	准确率@5  65.62 ( 57.81)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Epoch: [1][ 1/39]	Time 79.411 (79.411)	Data  7.969 ( 7.969)	损失 2.5353e+00 (2.5353e+00)	准确率@1  12.50 ( 12.50)	准确率@5  50.00 ( 50.00)
Epoch: [1][ 6/39]	Time  0.022 (26.044)	Data  0.002 (13.352)	损失 2.3880e+00 (2.5330e+00)	准确率@1   9.38 ( 10.42)	准确率@5  62.50 ( 51.56)
Epoch: [1][11/39]	Time  0.022 (14.217)	Data  0.002 ( 7.285)	损失 2.7379e+00 (2.5935e+00)	准确率@1   6.25 ( 11.36)	准确率@5  40.62 ( 50.28)
Epoch: [1][16/39]	Time  0.026 ( 9.782)	Data  0.006 ( 5.010)	损失 2.6763e+00 (2.5907e+00)	准确率@1  12.50 ( 10.55)	准确率@5  46.88 ( 50.00)
Epoch: [1][21/39]	Time  0.026 ( 7.459)	Data  0.005 ( 3.818)	损失 2.7166e+00 (2.6030e+00)	准确率@1   6.25 ( 10.27)	准确率@5  43.75 ( 49.85)
Epoch: [1][26/39]	Time  0.026 ( 6.030)	Data  0.006 ( 3.085)	损失 2.4558e+00 (2.5837e+00)	准确率@1  15.62 ( 11.06)	准确率@5  50.00 ( 50.12)
Epoch: [1][31/39]	Time  0.026 ( 5.061)	Data  0.005 ( 2.588)	损失 2.5382e+00 (2.5725e+00)	准确率@1   9.38 ( 10.58)	准确率@5  50.00 ( 50.71)
Epoch: [1][36/39]	Time  0.025 ( 4.362)	Data  0.004 ( 2.229)	损失 2.1866e+00 (2.5375e+00)	准确率@1  15.62 ( 11.63)	准确率@5  78.12 ( 52.43)
/root/miniconda3/envs/Pein38/lib/python3.8/site-packages/torch/autograd/__init__.py:200: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [10, 512], strides() = [512, 1]
bucket_view.sizes() = [5120], strides() = [1] (Triggered internally at torch_npu/csrc/distributed/reducer.cpp:315.)
  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
-\Test: [  1/156]	Time  8.899 ( 8.899)	Loss 2.3573e+00 (2.3573e+00)	Acc@1  18.75 ( 18.75)	Acc@5  62.50 ( 62.50)
Test: [  6/156]	Time  0.025 ( 1.497)	Loss 2.4869e+00 (2.2635e+00)	Acc@1  12.50 ( 20.83)	Acc@5  53.12 ( 65.62)
Test: [ 11/156]	Time  0.025 ( 0.828)	Loss 2.1482e+00 (2.2540e+00)	Acc@1  25.00 ( 19.89)	Acc@5  65.62 ( 65.62)
Test: [ 16/156]	Time  0.025 ( 0.577)	Loss 2.3278e+00 (2.2844e+00)	Acc@1  18.75 ( 18.16)	Acc@5  62.50 ( 64.65)
Test: [ 21/156]	Time  0.026 ( 0.446)	Loss 2.3327e+00 (2.3039e+00)	Acc@1  15.62 ( 17.41)	Acc@5  56.25 ( 64.29)
Test: [ 26/156]	Time  0.026 ( 0.365)	Loss 2.1434e+00 (2.3252e+00)	Acc@1  15.62 ( 16.83)	Acc@5  84.38 ( 63.58)
Test: [ 31/156]	Time  0.025 ( 0.310)	Loss 2.3245e+00 (2.3345e+00)	Acc@1  21.88 ( 16.43)	Acc@5  75.00 ( 63.61)
Test: [ 36/156]	Time  0.025 ( 0.271)	Loss 2.5147e+00 (2.3451e+00)	Acc@1  18.75 ( 16.58)	Acc@5  53.12 ( 63.63)
Test: [ 41/156]	Time  0.025 ( 0.241)	Loss 2.1404e+00 (2.3373e+00)	Acc@1  25.00 ( 16.77)	Acc@5  71.88 ( 63.80)
Test: [ 46/156]	Time  0.025 ( 0.217)	Loss 2.3079e+00 (2.3502e+00)	Acc@1  15.62 ( 16.44)	Acc@5  59.38 ( 63.45)
Test: [ 51/156]	Time  0.025 ( 0.198)	Loss 2.3554e+00 (2.3477e+00)	Acc@1  21.88 ( 16.73)	Acc@5  62.50 ( 63.48)
Test: [ 56/156]	Time  0.025 ( 0.183)	Loss 2.5022e+00 (2.3445e+00)	Acc@1  18.75 ( 16.80)	Acc@5  50.00 ( 63.17)
Test: [ 61/156]	Time  0.025 ( 0.170)	Loss 2.5035e+00 (2.3522e+00)	Acc@1   6.25 ( 16.24)	Acc@5  59.38 ( 63.01)
Test: [ 66/156]	Time  0.025 ( 0.159)	Loss 2.3035e+00 (2.3431e+00)	Acc@1  12.50 ( 16.34)	Acc@5  68.75 ( 63.73)
Test: [ 71/156]	Time  0.025 ( 0.150)	Loss 2.3862e+00 (2.3379e+00)	Acc@1   9.38 ( 16.37)	Acc@5  65.62 ( 64.08)
Test: [ 76/156]	Time  0.026 ( 0.141)	Loss 2.4209e+00 (2.3350e+00)	Acc@1  28.12 ( 16.49)	Acc@5  59.38 ( 64.19)
Test: [ 81/156]	Time  0.025 ( 0.134)	Loss 2.2826e+00 (2.3379e+00)	Acc@1  18.75 ( 16.55)	Acc@5  62.50 ( 63.89)
Test: [ 86/156]	Time  0.025 ( 0.128)	Loss 2.3775e+00 (2.3318e+00)	Acc@1   3.12 ( 16.32)	Acc@5  62.50 ( 64.10)
Test: [ 91/156]	Time  0.025 ( 0.122)	Loss 2.3398e+00 (2.3337e+00)	Acc@1  15.62 ( 16.11)	Acc@5  62.50 ( 64.18)
Test: [ 96/156]	Time  0.025 ( 0.117)	Loss 2.3464e+00 (2.3316e+00)	Acc@1  15.62 ( 16.02)	Acc@5  62.50 ( 64.52)
Test: [101/156]	Time  0.025 ( 0.113)	Loss 2.3384e+00 (2.3330e+00)	Acc@1  15.62 ( 15.87)	Acc@5  68.75 ( 64.51)
Test: [106/156]	Time  0.025 ( 0.109)	Loss 2.4989e+00 (2.3338e+00)	Acc@1  12.50 ( 15.89)	Acc@5  59.38 ( 64.27)
Test: [111/156]	Time  0.026 ( 0.105)	Loss 2.3356e+00 (2.3331e+00)	Acc@1  15.62 ( 16.05)	Acc@5  65.62 ( 64.22)
Test: [116/156]	Time  0.025 ( 0.101)	Loss 2.2547e+00 (2.3359e+00)	Acc@1  21.88 ( 16.08)	Acc@5  71.88 ( 64.12)
Test: [121/156]	Time  0.025 ( 0.098)	Loss 2.4250e+00 (2.3342e+00)	Acc@1   9.38 ( 16.19)	Acc@5  59.38 ( 64.28)
Test: [126/156]	Time  0.025 ( 0.095)	Loss 2.1553e+00 (2.3352e+00)	Acc@1  18.75 ( 16.10)	Acc@5  75.00 ( 64.31)
Test: [131/156]	Time  0.025 ( 0.093)	Loss 2.2530e+00 (2.3329e+00)	Acc@1  21.88 ( 16.05)	Acc@5  65.62 ( 64.43)
Test: [136/156]	Time  0.025 ( 0.090)	Loss 2.1269e+00 (2.3304e+00)	Acc@1  21.88 ( 16.13)	Acc@5  87.50 ( 64.61)
Test: [141/156]	Time  0.025 ( 0.088)	Loss 2.3687e+00 (2.3335e+00)	Acc@1  15.62 ( 16.05)	Acc@5  53.12 ( 64.65)
Test: [146/156]	Time  0.025 ( 0.086)	Loss 2.4080e+00 (2.3359e+00)	Acc@1   9.38 ( 15.99)	Acc@5  68.75 ( 64.62)
Test: [151/156]	Time  0.025 ( 0.084)	Loss 2.0608e+00 (2.3325e+00)	Acc@1  28.12 ( 16.06)	Acc@5  71.88 ( 64.82)
Test: [156/156]	Time  0.024 ( 0.082)	Loss 2.2041e+00 (2.3312e+00)	Acc@1  15.62 ( 15.99)	Acc@5  65.62 ( 64.82)
 * Time 0.082 Loss 2.331 Acc@1 15.986 Acc@5 64.824
saving checkpoint at GPU:0 
 
Epoch: [2][ 1/39]	Time  4.498 ( 4.498)	Data  4.475 ( 4.475)	损失 2.4215e+00 (2.4215e+00)	准确率@1  15.62 ( 15.62)	准确率@5  59.38 ( 59.38)
Epoch: [2][ 6/39]	Time  0.030 ( 3.288)	Data  0.001 ( 3.264)	损失 2.5074e+00 (2.3650e+00)	准确率@1  21.88 ( 13.54)	准确率@5  59.38 ( 67.19)
Epoch: [2][11/39]	Time  0.025 ( 1.805)	Data  0.001 ( 1.782)	损失 2.2953e+00 (2.3046e+00)	准确率@1  18.75 ( 15.62)	准确率@5  68.75 ( 67.05)
Epoch: [2][16/39]	Time  0.025 ( 1.249)	Data  0.002 ( 1.226)	损失 2.1431e+00 (2.2839e+00)	准确率@1  15.62 ( 15.62)	准确率@5  78.12 ( 68.55)
Epoch: [2][21/39]	Time  0.026 ( 0.958)	Data  0.002 ( 0.935)	损失 2.4454e+00 (2.2576e+00)	准确率@1  18.75 ( 17.71)	准确率@5  59.38 ( 69.05)
Epoch: [2][26/39]	Time  0.024 ( 0.778)	Data  0.003 ( 0.756)	损失 2.0395e+00 (2.2375e+00)	准确率@1  25.00 ( 18.03)	准确率@5  65.62 ( 69.23)
Epoch: [2][31/39]	Time  0.025 ( 0.657)	Data  0.002 ( 0.635)	损失 2.0938e+00 (2.2416e+00)	准确率@1  31.25 ( 18.95)	准确率@5  81.25 ( 69.35)
Epoch: [2][36/39]	Time  0.025 ( 0.569)	Data  0.002 ( 0.547)	损失 2.2930e+00 (2.2376e+00)	准确率@1  25.00 ( 19.62)	准确率@5  62.50 ( 69.44)
Epoch: [2][ 1/39]	Time  4.468 ( 4.468)	Data  4.445 ( 4.445)	损失 2.2062e+00 (2.2062e+00)	准确率@1  18.75 ( 18.75)	准确率@5  81.25 ( 81.25)
Epoch: [2][ 6/39]	Time  0.030 ( 3.292)	Data  0.002 ( 3.268)	损失 2.4096e+00 (2.2855e+00)	准确率@1  15.62 ( 17.19)	准确率@5  56.25 ( 68.23)
Epoch: [2][11/39]	Time  0.025 ( 1.807)	Data  0.002 ( 1.784)	损失 2.3411e+00 (2.2903e+00)	准确率@1  18.75 ( 16.48)	准确率@5  62.50 ( 67.05)
Epoch: [2][16/39]	Time  0.025 ( 1.250)	Data  0.002 ( 1.228)	损失 2.2260e+00 (2.2362e+00)	准确率@1  15.62 ( 18.95)	准确率@5  62.50 ( 67.97)
Epoch: [2][21/39]	Time  0.024 ( 0.959)	Data  0.002 ( 0.936)	损失 2.0895e+00 (2.2346e+00)	准确率@1  25.00 ( 19.64)	准确率@5  71.88 ( 67.86)
Epoch: [2][26/39]	Time  0.024 ( 0.779)	Data  0.002 ( 0.757)	损失 2.2806e+00 (2.2249e+00)	准确率@1  25.00 ( 19.83)	准确率@5  65.62 ( 68.63)
Epoch: [2][31/39]	Time  0.025 ( 0.658)	Data  0.002 ( 0.635)	损失 1.9681e+00 (2.2014e+00)	准确率@1  25.00 ( 20.56)	准确率@5  81.25 ( 69.25)
Epoch: [2][36/39]	Time  0.025 ( 0.570)	Data  0.002 ( 0.548)	损失 2.1729e+00 (2.2000e+00)	准确率@1  28.12 ( 21.09)	准确率@5  62.50 ( 69.18)
Epoch: [2][ 1/39]	Time  4.417 ( 4.417)	Data  4.394 ( 4.394)	损失 2.3594e+00 (2.3594e+00)	准确率@1  18.75 ( 18.75)	准确率@5  65.62 ( 65.62)
Epoch: [2][ 6/39]	Time  0.029 ( 3.297)	Data  0.002 ( 3.274)	损失 2.2411e+00 (2.3715e+00)	准确率@1  21.88 ( 18.23)	准确率@5  65.62 ( 60.94)
Epoch: [2][11/39]	Time  0.025 ( 1.810)	Data  0.002 ( 1.787)	损失 2.2765e+00 (2.3270e+00)	准确率@1  15.62 ( 16.19)	准确率@5  68.75 ( 63.35)
Epoch: [2][16/39]	Time  0.025 ( 1.252)	Data  0.002 ( 1.230)	损失 2.3669e+00 (2.2981e+00)	准确率@1  18.75 ( 17.58)	准确率@5  65.62 ( 65.43)
Epoch: [2][21/39]	Time  0.024 ( 0.960)	Data  0.002 ( 0.938)	损失 2.2467e+00 (2.2887e+00)	准确率@1  18.75 ( 17.11)	准确率@5  71.88 ( 67.11)
Epoch: [2][26/39]	Time  0.025 ( 0.780)	Data  0.002 ( 0.758)	损失 2.1742e+00 (2.2908e+00)	准确率@1  34.38 ( 17.91)	准确率@5  71.88 ( 66.59)
Epoch: [2][31/39]	Time  0.025 ( 0.659)	Data  0.002 ( 0.636)	损失 2.1285e+00 (2.2696e+00)	准确率@1  18.75 ( 18.25)	准确率@5  71.88 ( 67.64)
Epoch: [2][36/39]	Time  0.025 ( 0.571)	Data  0.002 ( 0.549)	损失 2.0553e+00 (2.2530e+00)	准确率@1  15.62 ( 18.23)	准确率@5  84.38 ( 68.49)
Epoch: [2][ 1/39]	Time  4.617 ( 4.617)	Data  4.591 ( 4.591)	损失 2.5726e+00 (2.5726e+00)	准确率@1   9.38 (  9.38)	准确率@5  50.00 ( 50.00)
Epoch: [2][ 6/39]	Time  0.029 ( 3.293)	Data  0.002 ( 3.270)	损失 2.1248e+00 (2.3793e+00)	准确率@1  25.00 ( 16.15)	准确率@5  71.88 ( 59.38)
Epoch: [2][11/39]	Time  0.026 ( 1.808)	Data  0.002 ( 1.785)	损失 2.3605e+00 (2.3591e+00)	准确率@1   9.38 ( 16.19)	准确率@5  56.25 ( 62.22)
Epoch: [2][16/39]	Time  0.026 ( 1.251)	Data  0.002 ( 1.228)	损失 2.3473e+00 (2.3639e+00)	准确率@1  12.50 ( 15.23)	准确率@5  71.88 ( 64.06)
Epoch: [2][21/39]	Time  0.025 ( 0.959)	Data  0.002 ( 0.937)	损失 2.5490e+00 (2.3469e+00)	准确率@1   9.38 ( 15.92)	准确率@5  68.75 ( 65.03)
Epoch: [2][26/39]	Time  0.025 ( 0.780)	Data  0.003 ( 0.757)	损失 2.2552e+00 (2.3181e+00)	准确率@1  12.50 ( 16.35)	准确率@5  71.88 ( 66.95)
Epoch: [2][31/39]	Time  0.025 ( 0.658)	Data  0.002 ( 0.636)	损失 2.3033e+00 (2.3053e+00)	准确率@1  12.50 ( 16.83)	准确率@5  65.62 ( 67.04)
Epoch: [2][36/39]	Time  0.024 ( 0.570)	Data  0.002 ( 0.548)	损失 2.4827e+00 (2.2949e+00)	准确率@1  15.62 ( 17.45)	准确率@5  59.38 ( 67.19)
Epoch: [2][ 1/39]	Time  4.556 ( 4.556)	Data  4.533 ( 4.533)	损失 2.2876e+00 (2.2876e+00)	准确率@1  15.62 ( 15.62)	准确率@5  68.75 ( 68.75)
Epoch: [2][ 6/39]	Time  0.029 ( 3.286)	Data  0.002 ( 3.263)	损失 2.0567e+00 (2.2872e+00)	准确率@1  21.88 ( 16.15)	准确率@5  71.88 ( 66.67)
Epoch: [2][11/39]	Time  0.026 ( 1.804)	Data  0.002 ( 1.782)	损失 2.0602e+00 (2.2616e+00)	准确率@1  21.88 ( 16.76)	准确率@5  81.25 ( 67.33)
Epoch: [2][16/39]	Time  0.026 ( 1.248)	Data  0.002 ( 1.226)	损失 2.3310e+00 (2.2576e+00)	准确率@1  12.50 ( 18.75)	准确率@5  71.88 ( 67.58)
Epoch: [2][21/39]	Time  0.024 ( 0.957)	Data  0.002 ( 0.935)	损失 2.1012e+00 (2.2334e+00)	准确率@1  31.25 ( 19.64)	准确率@5  78.12 ( 69.05)
Epoch: [2][26/39]	Time  0.025 ( 0.778)	Data  0.003 ( 0.756)	损失 2.3084e+00 (2.2421e+00)	准确率@1   9.38 ( 18.99)	准确率@5  65.62 ( 68.63)
Epoch: [2][31/39]	Time  0.025 ( 0.657)	Data  0.002 ( 0.634)	损失 1.9394e+00 (2.2182e+00)	准确率@1  31.25 ( 19.96)	准确率@5  87.50 ( 70.06)
Epoch: [2][36/39]	Time  0.024 ( 0.569)	Data  0.002 ( 0.547)	损失 2.2389e+00 (2.2041e+00)	准确率@1  15.62 ( 21.09)	准确率@5  65.62 ( 70.05)
Epoch: [2][ 1/39]	Time  4.617 ( 4.617)	Data  4.590 ( 4.590)	损失 2.5202e+00 (2.5202e+00)	准确率@1   9.38 (  9.38)	准确率@5  56.25 ( 56.25)
Epoch: [2][ 6/39]	Time  0.029 ( 3.296)	Data  0.002 ( 3.272)	损失 2.2627e+00 (2.3897e+00)	准确率@1  15.62 ( 11.98)	准确率@5  68.75 ( 60.42)
Epoch: [2][11/39]	Time  0.025 ( 1.810)	Data  0.002 ( 1.786)	损失 2.4046e+00 (2.4050e+00)	准确率@1  18.75 ( 13.64)	准确率@5  56.25 ( 61.65)
Epoch: [2][16/39]	Time  0.025 ( 1.252)	Data  0.002 ( 1.229)	损失 2.4716e+00 (2.3668e+00)	准确率@1   6.25 ( 12.89)	准确率@5  50.00 ( 63.28)
Epoch: [2][21/39]	Time  0.025 ( 0.960)	Data  0.002 ( 0.937)	损失 2.5813e+00 (2.3523e+00)	准确率@1  15.62 ( 14.29)	准确率@5  46.88 ( 63.39)
Epoch: [2][26/39]	Time  0.025 ( 0.780)	Data  0.003 ( 0.757)	损失 2.1170e+00 (2.3456e+00)	准确率@1  28.12 ( 15.26)	准确率@5  78.12 ( 63.82)
Epoch: [2][31/39]	Time  0.025 ( 0.659)	Data  0.002 ( 0.635)	损失 2.2436e+00 (2.3179e+00)	准确率@1  25.00 ( 15.83)	准确率@5  71.88 ( 65.52)
Epoch: [2][36/39]	Time  0.025 ( 0.571)	Data  0.002 ( 0.547)	损失 2.1917e+00 (2.2996e+00)	准确率@1  31.25 ( 17.27)	准确率@5  71.88 ( 65.89)
Epoch: [2][ 1/39]	Time  4.400 ( 4.400)	Data  4.376 ( 4.376)	损失 2.2999e+00 (2.2999e+00)	准确率@1  12.50 ( 12.50)	准确率@5  71.88 ( 71.88)
Epoch: [2][ 6/39]	Time  0.026 ( 0.756)	Data  0.002 ( 0.734)	损失 2.2825e+00 (2.2429e+00)	准确率@1  25.00 ( 19.27)	准确率@5  75.00 ( 72.40)
Epoch: [2][11/39]	Time  0.025 ( 0.424)	Data  0.002 ( 0.401)	损失 2.3618e+00 (2.3175e+00)	准确率@1  18.75 ( 18.75)	准确率@5  62.50 ( 67.61)
Epoch: [2][16/39]	Time  0.025 ( 0.299)	Data  0.002 ( 0.276)	损失 2.1978e+00 (2.2938e+00)	准确率@1  18.75 ( 19.34)	准确率@5  75.00 ( 68.95)
Epoch: [2][21/39]	Time  0.025 ( 0.234)	Data  0.002 ( 0.211)	损失 2.3907e+00 (2.2993e+00)	准确率@1  31.25 ( 19.20)	准确率@5  59.38 ( 68.30)
Epoch: [2][26/39]	Time  0.029 ( 0.194)	Data  0.005 ( 0.171)	损失 2.1093e+00 (2.2673e+00)	准确率@1  18.75 ( 19.95)	准确率@5  75.00 ( 69.59)
Epoch: [2][31/39]	Time  0.025 ( 0.167)	Data  0.002 ( 0.144)	损失 2.3540e+00 (2.2479e+00)	准确率@1  21.88 ( 20.77)	准确率@5  62.50 ( 69.56)
Epoch: [2][36/39]	Time  0.025 ( 0.147)	Data  0.002 ( 0.124)	损失 1.8874e+00 (2.2220e+00)	准确率@1  34.38 ( 21.61)	准确率@5  81.25 ( 70.57)
Epoch: [2][ 1/39]	Time  4.699 ( 4.699)	Data  4.671 ( 4.671)	损失 2.4716e+00 (2.4716e+00)	准确率@1  12.50 ( 12.50)	准确率@5  46.88 ( 46.88)
Epoch: [2][ 6/39]	Time  0.029 ( 3.301)	Data  0.002 ( 3.277)	损失 2.1421e+00 (2.3011e+00)	准确率@1  18.75 ( 17.19)	准确率@5  81.25 ( 64.58)
Epoch: [2][11/39]	Time  0.025 ( 1.812)	Data  0.002 ( 1.789)	损失 2.0994e+00 (2.2758e+00)	准确率@1  25.00 ( 18.47)	准确率@5  71.88 ( 65.91)
Epoch: [2][16/39]	Time  0.023 ( 1.254)	Data  0.002 ( 1.231)	损失 2.2344e+00 (2.2662e+00)	准确率@1  21.88 ( 18.75)	准确率@5  68.75 ( 66.99)
Epoch: [2][21/39]	Time  0.024 ( 0.961)	Data  0.002 ( 0.939)	损失 2.1546e+00 (2.2715e+00)	准确率@1  21.88 ( 18.75)	准确率@5  68.75 ( 66.82)
Epoch: [2][26/39]	Time  0.024 ( 0.781)	Data  0.003 ( 0.759)	损失 1.9738e+00 (2.2415e+00)	准确率@1  37.50 ( 20.91)	准确率@5  78.12 ( 67.79)
Epoch: [2][31/39]	Time  0.025 ( 0.659)	Data  0.003 ( 0.637)	损失 2.1392e+00 (2.2367e+00)	准确率@1  31.25 ( 21.17)	准确率@5  78.12 ( 68.04)
Epoch: [2][36/39]	Time  0.025 ( 0.571)	Data  0.002 ( 0.549)	损失 2.2199e+00 (2.2145e+00)	准确率@1  21.88 ( 21.44)	准确率@5  65.62 ( 69.18)
|Test: [  1/156]	Time  4.645 ( 4.645)	Loss 2.1641e+00 (2.1641e+00)	Acc@1  34.38 ( 34.38)	Acc@5  68.75 ( 68.75)
Test: [  6/156]	Time  0.024 ( 0.794)	Loss 2.2409e+00 (2.1097e+00)	Acc@1  12.50 ( 23.96)	Acc@5  75.00 ( 75.00)
Test: [ 11/156]	Time  0.024 ( 0.444)	Loss 1.7945e+00 (2.0390e+00)	Acc@1  34.38 ( 24.72)	Acc@5  84.38 ( 76.14)
Test: [ 16/156]	Time  0.024 ( 0.313)	Loss 2.0412e+00 (2.0457e+00)	Acc@1  25.00 ( 25.39)	Acc@5  78.12 ( 76.37)
Test: [ 21/156]	Time  0.023 ( 0.244)	Loss 2.1765e+00 (2.0851e+00)	Acc@1  25.00 ( 24.11)	Acc@5  68.75 ( 75.30)
Test: [ 26/156]	Time  0.024 ( 0.201)	Loss 2.1513e+00 (2.1072e+00)	Acc@1  18.75 ( 23.20)	Acc@5  62.50 ( 74.16)
Test: [ 31/156]	Time  0.024 ( 0.173)	Loss 1.8451e+00 (2.1043e+00)	Acc@1  28.12 ( 23.08)	Acc@5  87.50 ( 73.99)
Test: [ 36/156]	Time  0.025 ( 0.152)	Loss 2.2830e+00 (2.1208e+00)	Acc@1  21.88 ( 23.18)	Acc@5  62.50 ( 73.96)
Test: [ 41/156]	Time  0.025 ( 0.137)	Loss 2.1139e+00 (2.1239e+00)	Acc@1  31.25 ( 22.87)	Acc@5  78.12 ( 74.39)
Test: [ 46/156]	Time  0.024 ( 0.125)	Loss 2.3216e+00 (2.1304e+00)	Acc@1   9.38 ( 22.62)	Acc@5  68.75 ( 74.80)
Test: [ 51/156]	Time  0.025 ( 0.115)	Loss 2.0082e+00 (2.1225e+00)	Acc@1  34.38 ( 22.73)	Acc@5  68.75 ( 74.63)
Test: [ 56/156]	Time  0.025 ( 0.107)	Loss 2.0045e+00 (2.1210e+00)	Acc@1  31.25 ( 23.21)	Acc@5  71.88 ( 74.78)
Test: [ 61/156]	Time  0.025 ( 0.100)	Loss 2.1309e+00 (2.1257e+00)	Acc@1  15.62 ( 23.00)	Acc@5  68.75 ( 74.49)
Test: [ 66/156]	Time  0.025 ( 0.094)	Loss 1.9287e+00 (2.1180e+00)	Acc@1  31.25 ( 23.30)	Acc@5  87.50 ( 75.05)
Test: [ 71/156]	Time  0.024 ( 0.089)	Loss 2.1365e+00 (2.1117e+00)	Acc@1  28.12 ( 23.68)	Acc@5  78.12 ( 75.40)
Test: [ 76/156]	Time  0.025 ( 0.085)	Loss 2.0199e+00 (2.1155e+00)	Acc@1  28.12 ( 23.48)	Acc@5  78.12 ( 75.29)
Test: [ 81/156]	Time  0.025 ( 0.081)	Loss 2.0486e+00 (2.1243e+00)	Acc@1  25.00 ( 23.30)	Acc@5  81.25 ( 74.96)
Test: [ 86/156]	Time  0.025 ( 0.078)	Loss 2.2168e+00 (2.1170e+00)	Acc@1  21.88 ( 23.58)	Acc@5  68.75 ( 75.22)
Test: [ 91/156]	Time  0.025 ( 0.075)	Loss 2.0981e+00 (2.1139e+00)	Acc@1  25.00 ( 23.56)	Acc@5  65.62 ( 75.27)
Test: [ 96/156]	Time  0.025 ( 0.073)	Loss 2.1102e+00 (2.1102e+00)	Acc@1  31.25 ( 23.60)	Acc@5  71.88 ( 75.46)
Test: [101/156]	Time  0.025 ( 0.070)	Loss 2.1108e+00 (2.1111e+00)	Acc@1  25.00 ( 23.67)	Acc@5  75.00 ( 75.50)
Test: [106/156]	Time  0.025 ( 0.068)	Loss 2.1005e+00 (2.1107e+00)	Acc@1  28.12 ( 23.79)	Acc@5  65.62 ( 75.38)
Test: [111/156]	Time  0.025 ( 0.066)	Loss 2.1479e+00 (2.1160e+00)	Acc@1  21.88 ( 23.56)	Acc@5  81.25 ( 75.00)
Test: [116/156]	Time  0.025 ( 0.064)	Loss 2.1831e+00 (2.1166e+00)	Acc@1  21.88 ( 23.63)	Acc@5  68.75 ( 75.08)
Test: [121/156]	Time  0.025 ( 0.063)	Loss 2.1052e+00 (2.1181e+00)	Acc@1  12.50 ( 23.68)	Acc@5  81.25 ( 74.97)
Test: [126/156]	Time  0.025 ( 0.061)	Loss 1.9832e+00 (2.1196e+00)	Acc@1  28.12 ( 23.51)	Acc@5  84.38 ( 74.90)
Test: [131/156]	Time  0.025 ( 0.060)	Loss 2.0893e+00 (2.1156e+00)	Acc@1  21.88 ( 23.59)	Acc@5  68.75 ( 75.05)
Test: [136/156]	Time  0.025 ( 0.059)	Loss 1.9965e+00 (2.1121e+00)	Acc@1  28.12 ( 23.69)	Acc@5  75.00 ( 75.18)
Test: [141/156]	Time  0.025 ( 0.057)	Loss 1.9908e+00 (2.1124e+00)	Acc@1  31.25 ( 23.67)	Acc@5  75.00 ( 75.13)
Test: [146/156]	Time  0.026 ( 0.056)	Loss 2.1775e+00 (2.1117e+00)	Acc@1  31.25 ( 23.74)	Acc@5  68.75 ( 75.19)
Test: [151/156]	Time  0.025 ( 0.055)	Loss 1.9732e+00 (2.1107e+00)	Acc@1  21.88 ( 23.70)	Acc@5  81.25 ( 75.33)
Test: [156/156]	Time  0.024 ( 0.054)	Loss 2.1061e+00 (2.1103e+00)	Acc@1  25.00 ( 23.76)	Acc@5  75.00 ( 75.32)
 * Time 0.054 Loss 2.110 Acc@1 23.758 Acc@5 75.321
saving checkpoint at GPU:0 
 
Epoch: [3][ 1/39]	Time  4.552 ( 4.552)	Data  4.529 ( 4.529)	损失 2.0625e+00 (2.0625e+00)	准确率@1  31.25 ( 31.25)	准确率@5  68.75 ( 68.75)
Epoch: [3][ 6/39]	Time  0.024 ( 2.372)	Data  0.002 ( 2.349)	损失 2.0252e+00 (2.0659e+00)	准确率@1  25.00 ( 30.73)	准确率@5  84.38 ( 73.96)
Epoch: [3][11/39]	Time  0.027 ( 1.306)	Data  0.001 ( 1.283)	损失 1.9574e+00 (2.0371e+00)	准确率@1  37.50 ( 29.83)	准确率@5  78.12 ( 74.72)
Epoch: [3][16/39]	Time  0.027 ( 0.906)	Data  0.001 ( 0.883)	损失 1.8024e+00 (2.0327e+00)	准确率@1  43.75 ( 28.91)	准确率@5  87.50 ( 75.98)
Epoch: [3][21/39]	Time  0.027 ( 0.696)	Data  0.001 ( 0.674)	损失 1.8668e+00 (2.0146e+00)	准确率@1  28.12 ( 28.72)	准确率@5  87.50 ( 77.53)
Epoch: [3][26/39]	Time  0.027 ( 0.568)	Data  0.001 ( 0.545)	损失 2.0497e+00 (2.0019e+00)	准确率@1  37.50 ( 30.05)	准确率@5  78.12 ( 78.12)
Epoch: [3][31/39]	Time  0.026 ( 0.481)	Data  0.002 ( 0.457)	损失 1.9445e+00 (2.0062e+00)	准确率@1  31.25 ( 28.93)	准确率@5  78.12 ( 78.12)
Epoch: [3][36/39]	Time  0.027 ( 0.417)	Data  0.002 ( 0.394)	损失 1.9755e+00 (1.9944e+00)	准确率@1  21.88 ( 29.25)	准确率@5  87.50 ( 78.91)
Epoch: [3][ 1/39]	Time  4.489 ( 4.489)	Data  4.465 ( 4.465)	损失 2.0173e+00 (2.0173e+00)	准确率@1  18.75 ( 18.75)	准确率@5  81.25 ( 81.25)
Epoch: [3][ 6/39]	Time  0.024 ( 2.334)	Data  0.002 ( 2.312)	损失 2.3097e+00 (2.1542e+00)	准确率@1  21.88 ( 29.17)	准确率@5  62.50 ( 72.40)
Epoch: [3][11/39]	Time  0.027 ( 1.285)	Data  0.002 ( 1.263)	损失 2.1297e+00 (2.1309e+00)	准确率@1  25.00 ( 28.12)	准确率@5  71.88 ( 72.44)
Epoch: [3][16/39]	Time  0.026 ( 0.892)	Data  0.002 ( 0.869)	损失 2.0742e+00 (2.0984e+00)	准确率@1  37.50 ( 28.32)	准确率@5  81.25 ( 74.02)
Epoch: [3][21/39]	Time  0.026 ( 0.686)	Data  0.002 ( 0.663)	损失 1.9853e+00 (2.0811e+00)	准确率@1  28.12 ( 28.87)	准确率@5  78.12 ( 73.96)
Epoch: [3][26/39]	Time  0.026 ( 0.559)	Data  0.002 ( 0.536)	损失 1.9329e+00 (2.0742e+00)	准确率@1  28.12 ( 28.61)	准确率@5  81.25 ( 74.64)
Epoch: [3][31/39]	Time  0.026 ( 0.473)	Data  0.002 ( 0.450)	损失 2.1613e+00 (2.0669e+00)	准确率@1  31.25 ( 28.33)	准确率@5  68.75 ( 74.60)
Epoch: [3][36/39]	Time  0.026 ( 0.411)	Data  0.002 ( 0.388)	损失 2.0037e+00 (2.0462e+00)	准确率@1  28.12 ( 28.99)	准确率@5  81.25 ( 75.17)
Epoch: [3][ 1/39]	Time  4.538 ( 4.538)	Data  4.511 ( 4.511)	损失 1.9184e+00 (1.9184e+00)	准确率@1  34.38 ( 34.38)	准确率@5  87.50 ( 87.50)
Epoch: [3][ 6/39]	Time  0.024 ( 2.372)	Data  0.002 ( 2.347)	损失 1.9360e+00 (2.0707e+00)	准确率@1  28.12 ( 28.12)	准确率@5  81.25 ( 77.60)
Epoch: [3][11/39]	Time  0.026 ( 1.305)	Data  0.002 ( 1.281)	损失 2.1540e+00 (2.1032e+00)	准确率@1  21.88 ( 25.00)	准确率@5  71.88 ( 73.86)
Epoch: [3][16/39]	Time  0.026 ( 0.906)	Data  0.002 ( 0.882)	损失 2.2076e+00 (2.0577e+00)	准确率@1  12.50 ( 25.98)	准确率@5  71.88 ( 74.80)
Epoch: [3][21/39]	Time  0.025 ( 0.696)	Data  0.002 ( 0.673)	损失 2.0295e+00 (2.0461e+00)	准确率@1  31.25 ( 26.93)	准确率@5  71.88 ( 75.30)
Epoch: [3][26/39]	Time  0.027 ( 0.568)	Data  0.002 ( 0.544)	损失 1.8877e+00 (2.0445e+00)	准确率@1  28.12 ( 27.04)	准确率@5  87.50 ( 76.20)
Epoch: [3][31/39]	Time  0.026 ( 0.480)	Data  0.002 ( 0.456)	损失 1.8225e+00 (2.0353e+00)	准确率@1  31.25 ( 27.52)	准确率@5  90.62 ( 76.41)
Epoch: [3][36/39]	Time  0.027 ( 0.417)	Data  0.002 ( 0.394)	损失 2.0069e+00 (2.0193e+00)	准确率@1  28.12 ( 27.43)	准确率@5  81.25 ( 77.34)
Epoch: [3][ 1/39]	Time  4.646 ( 4.646)	Data  4.619 ( 4.619)	损失 2.1831e+00 (2.1831e+00)	准确率@1  28.12 ( 28.12)	准确率@5  71.88 ( 71.88)
Epoch: [3][ 6/39]	Time  0.024 ( 2.368)	Data  0.002 ( 2.345)	损失 2.1194e+00 (2.1065e+00)	准确率@1  31.25 ( 23.96)	准确率@5  78.12 ( 75.52)
Epoch: [3][11/39]	Time  0.027 ( 1.303)	Data  0.002 ( 1.280)	损失 1.9433e+00 (2.1000e+00)	准确率@1  43.75 ( 25.85)	准确率@5  78.12 ( 74.72)
Epoch: [3][16/39]	Time  0.026 ( 0.904)	Data  0.002 ( 0.881)	损失 1.8774e+00 (2.0587e+00)	准确率@1  37.50 ( 27.34)	准确率@5  81.25 ( 76.56)
Epoch: [3][21/39]	Time  0.027 ( 0.695)	Data  0.002 ( 0.672)	损失 2.0342e+00 (2.0683e+00)	准确率@1  28.12 ( 27.68)	准确率@5  81.25 ( 76.34)
Epoch: [3][26/39]	Time  0.027 ( 0.567)	Data  0.002 ( 0.544)	损失 1.8038e+00 (2.0463e+00)	准确率@1  43.75 ( 28.97)	准确率@5  90.62 ( 77.64)
Epoch: [3][31/39]	Time  0.026 ( 0.480)	Data  0.002 ( 0.456)	损失 1.9245e+00 (2.0222e+00)	准确率@1  28.12 ( 29.33)	准确率@5  84.38 ( 78.12)
Epoch: [3][36/39]	Time  0.027 ( 0.417)	Data  0.002 ( 0.394)	损失 1.8632e+00 (2.0234e+00)	准确率@1  43.75 ( 28.82)	准确率@5  78.12 ( 78.21)
Epoch: [3][ 1/39]	Time  4.767 ( 4.767)	Data  4.744 ( 4.744)	损失 2.0591e+00 (2.0591e+00)	准确率@1  28.12 ( 28.12)	准确率@5  87.50 ( 87.50)
Epoch: [3][ 6/39]	Time  0.024 ( 2.368)	Data  0.002 ( 2.345)	损失 2.2318e+00 (2.1359e+00)	准确率@1  18.75 ( 23.96)	准确率@5  75.00 ( 77.08)
Epoch: [3][11/39]	Time  0.027 ( 1.303)	Data  0.002 ( 1.281)	损失 1.7830e+00 (2.0668e+00)	准确率@1  43.75 ( 25.85)	准确率@5  84.38 ( 79.26)
Epoch: [3][16/39]	Time  0.027 ( 0.904)	Data  0.002 ( 0.882)	损失 1.8907e+00 (2.0565e+00)	准确率@1  31.25 ( 25.59)	准确率@5  84.38 ( 79.49)
Epoch: [3][21/39]	Time  0.027 ( 0.695)	Data  0.002 ( 0.673)	损失 1.9941e+00 (2.0435e+00)	准确率@1  21.88 ( 25.15)	准确率@5  78.12 ( 79.46)
Epoch: [3][26/39]	Time  0.027 ( 0.567)	Data  0.002 ( 0.544)	损失 2.0915e+00 (2.0339e+00)	准确率@1  28.12 ( 25.48)	准确率@5  84.38 ( 80.05)
Epoch: [3][31/39]	Time  0.026 ( 0.480)	Data  0.002 ( 0.457)	损失 1.8246e+00 (2.0270e+00)	准确率@1  34.38 ( 25.71)	准确率@5  87.50 ( 79.74)
Epoch: [3][36/39]	Time  0.027 ( 0.417)	Data  0.002 ( 0.394)	损失 2.1061e+00 (2.0155e+00)	准确率@1  12.50 ( 25.87)	准确率@5  71.88 ( 79.69)
Epoch: [3][ 1/39]	Time  4.512 ( 4.512)	Data  4.489 ( 4.489)	损失 2.1803e+00 (2.1803e+00)	准确率@1  21.88 ( 21.88)	准确率@5  71.88 ( 71.88)
Epoch: [3][ 6/39]	Time  0.024 ( 2.372)	Data  0.002 ( 2.349)	损失 1.9364e+00 (2.0428e+00)	准确率@1  25.00 ( 26.56)	准确率@5  81.25 ( 78.12)
Epoch: [3][11/39]	Time  0.027 ( 1.305)	Data  0.002 ( 1.283)	损失 1.9797e+00 (2.0064e+00)	准确率@1  31.25 ( 28.98)	准确率@5  87.50 ( 79.26)
Epoch: [3][16/39]	Time  0.027 ( 0.906)	Data  0.002 ( 0.883)	损失 2.1700e+00 (2.0205e+00)	准确率@1  25.00 ( 26.76)	准确率@5  68.75 ( 78.12)
Epoch: [3][21/39]	Time  0.026 ( 0.696)	Data  0.002 ( 0.674)	损失 2.1796e+00 (2.0402e+00)	准确率@1  15.62 ( 25.89)	准确率@5  71.88 ( 77.38)
Epoch: [3][26/39]	Time  0.027 ( 0.568)	Data  0.002 ( 0.545)	损失 1.8731e+00 (2.0145e+00)	准确率@1  28.12 ( 27.04)	准确率@5  90.62 ( 78.73)
Epoch: [3][31/39]	Time  0.026 ( 0.480)	Data  0.002 ( 0.457)	损失 2.0371e+00 (2.0179e+00)	准确率@1  18.75 ( 26.71)	准确率@5  75.00 ( 77.82)
Epoch: [3][36/39]	Time  0.027 ( 0.417)	Data  0.002 ( 0.394)	损失 1.7862e+00 (2.0004e+00)	准确率@1  43.75 ( 27.52)	准确率@5  87.50 ( 78.30)
Epoch: [3][ 1/39]	Time  4.488 ( 4.488)	Data  4.464 ( 4.464)	损失 2.1859e+00 (2.1859e+00)	准确率@1  21.88 ( 21.88)	准确率@5  68.75 ( 68.75)
Epoch: [3][ 6/39]	Time  0.026 ( 0.770)	Data  0.005 ( 0.748)	损失 2.0263e+00 (2.1773e+00)	准确率@1  31.25 ( 21.35)	准确率@5  75.00 ( 69.79)
Epoch: [3][11/39]	Time  0.026 ( 0.432)	Data  0.005 ( 0.410)	损失 1.9607e+00 (2.0956e+00)	准确率@1  31.25 ( 26.42)	准确率@5  81.25 ( 75.00)
Epoch: [3][16/39]	Time  0.026 ( 0.305)	Data  0.005 ( 0.284)	损失 2.0435e+00 (2.0960e+00)	准确率@1  15.62 ( 25.20)	准确率@5  78.12 ( 75.39)
Epoch: [3][21/39]	Time  0.026 ( 0.239)	Data  0.006 ( 0.218)	损失 2.0627e+00 (2.0915e+00)	准确率@1  34.38 ( 24.85)	准确率@5  81.25 ( 75.00)
Epoch: [3][26/39]	Time  0.025 ( 0.198)	Data  0.005 ( 0.177)	损失 1.9195e+00 (2.0696e+00)	准确率@1  37.50 ( 26.32)	准确率@5  78.12 ( 75.12)
Epoch: [3][31/39]	Time  0.026 ( 0.170)	Data  0.006 ( 0.149)	损失 1.9725e+00 (2.0683e+00)	准确率@1  31.25 ( 26.41)	准确率@5  84.38 ( 75.30)
Epoch: [3][36/39]	Time  0.026 ( 0.150)	Data  0.005 ( 0.129)	损失 1.8925e+00 (2.0538e+00)	准确率@1  34.38 ( 27.08)	准确率@5  81.25 ( 75.95)
Epoch: [3][ 1/39]	Time  4.621 ( 4.621)	Data  4.600 ( 4.600)	损失 2.2212e+00 (2.2212e+00)	准确率@1  31.25 ( 31.25)	准确率@5  75.00 ( 75.00)
Epoch: [3][ 6/39]	Time  0.025 ( 2.376)	Data  0.002 ( 2.354)	损失 1.9616e+00 (2.0961e+00)	准确率@1  34.38 ( 29.69)	准确率@5  84.38 ( 76.04)
Epoch: [3][11/39]	Time  0.026 ( 1.308)	Data  0.001 ( 1.285)	损失 2.1320e+00 (2.1107e+00)	准确率@1  15.62 ( 23.86)	准确率@5  78.12 ( 76.14)
Epoch: [3][16/39]	Time  0.026 ( 0.907)	Data  0.002 ( 0.885)	损失 1.6950e+00 (2.0772e+00)	准确率@1  46.88 ( 24.61)	准确率@5  96.88 ( 76.95)
Epoch: [3][21/39]	Time  0.027 ( 0.698)	Data  0.002 ( 0.675)	损失 2.0940e+00 (2.0669e+00)	准确率@1  28.12 ( 25.15)	准确率@5  81.25 ( 77.38)
Epoch: [3][26/39]	Time  0.026 ( 0.569)	Data  0.001 ( 0.546)	损失 1.7513e+00 (2.0511e+00)	准确率@1  37.50 ( 25.72)	准确率@5  84.38 ( 77.88)
Epoch: [3][31/39]	Time  0.026 ( 0.481)	Data  0.001 ( 0.458)	损失 1.8372e+00 (2.0384e+00)	准确率@1  28.12 ( 25.81)	准确率@5  84.38 ( 78.23)
Epoch: [3][36/39]	Time  0.026 ( 0.418)	Data  0.001 ( 0.395)	损失 1.9615e+00 (2.0299e+00)	准确率@1  25.00 ( 26.04)	准确率@5  75.00 ( 78.39)
Test: [  1/156]	Time  4.427 ( 4.427)	Loss 1.8432e+00 (1.8432e+00)	Acc@1  37.50 ( 37.50)	Acc@5  84.38 ( 84.38)
Test: [  6/156]	Time  0.024 ( 0.757)	Loss 2.1022e+00 (1.8922e+00)	Acc@1  18.75 ( 32.81)	Acc@5  75.00 ( 83.33)
Test: [ 11/156]	Time  0.024 ( 0.424)	Loss 1.8020e+00 (1.8613e+00)	Acc@1  43.75 ( 36.08)	Acc@5  81.25 ( 81.82)
Test: [ 16/156]	Time  0.026 ( 0.299)	Loss 1.9880e+00 (1.8589e+00)	Acc@1  31.25 ( 36.33)	Acc@5  81.25 ( 80.66)
Test: [ 21/156]	Time  0.025 ( 0.234)	Loss 1.8990e+00 (1.8692e+00)	Acc@1  25.00 ( 34.08)	Acc@5  84.38 ( 80.65)
Test: [ 26/156]	Time  0.026 ( 0.194)	Loss 1.7757e+00 (1.8840e+00)	Acc@1  40.62 ( 33.65)	Acc@5  84.38 ( 80.89)
Test: [ 31/156]	Time  0.025 ( 0.167)	Loss 1.6835e+00 (1.8859e+00)	Acc@1  40.62 ( 33.67)	Acc@5  93.75 ( 81.35)
Test: [ 36/156]	Time  0.025 ( 0.147)	Loss 2.0293e+00 (1.8944e+00)	Acc@1  31.25 ( 33.94)	Acc@5  75.00 ( 80.99)
Test: [ 41/156]	Time  0.025 ( 0.132)	Loss 1.8986e+00 (1.8959e+00)	Acc@1  31.25 ( 34.15)	Acc@5  78.12 ( 81.17)
Test: [ 46/156]	Time  0.025 ( 0.121)	Loss 1.9802e+00 (1.8979e+00)	Acc@1  40.62 ( 34.51)	Acc@5  84.38 ( 81.32)
Test: [ 51/156]	Time  0.025 ( 0.111)	Loss 2.0099e+00 (1.8933e+00)	Acc@1  37.50 ( 34.44)	Acc@5  81.25 ( 81.62)
Test: [ 56/156]	Time  0.025 ( 0.103)	Loss 1.8279e+00 (1.8907e+00)	Acc@1  46.88 ( 34.26)	Acc@5  87.50 ( 81.75)
Test: [ 61/156]	Time  0.025 ( 0.097)	Loss 1.7816e+00 (1.8939e+00)	Acc@1  31.25 ( 34.02)	Acc@5  84.38 ( 81.56)
Test: [ 66/156]	Time  0.025 ( 0.092)	Loss 1.9103e+00 (1.8871e+00)	Acc@1  34.38 ( 33.95)	Acc@5  81.25 ( 81.96)
Test: [ 71/156]	Time  0.025 ( 0.087)	Loss 2.2415e+00 (1.8885e+00)	Acc@1  15.62 ( 33.67)	Acc@5  71.88 ( 82.00)
Test: [ 76/156]	Time  0.025 ( 0.083)	Loss 2.0788e+00 (1.8921e+00)	Acc@1  31.25 ( 33.43)	Acc@5  71.88 ( 81.91)
Test: [ 81/156]	Time  0.025 ( 0.079)	Loss 1.9104e+00 (1.8939e+00)	Acc@1  40.62 ( 33.49)	Acc@5  81.25 ( 82.06)
Test: [ 86/156]	Time  0.025 ( 0.076)	Loss 2.2219e+00 (1.8931e+00)	Acc@1  12.50 ( 33.43)	Acc@5  71.88 ( 81.87)
Test: [ 91/156]	Time  0.025 ( 0.073)	Loss 1.9795e+00 (1.8935e+00)	Acc@1  34.38 ( 33.45)	Acc@5  81.25 ( 81.90)
Test: [ 96/156]	Time  0.025 ( 0.071)	Loss 1.8409e+00 (1.8928e+00)	Acc@1  40.62 ( 33.30)	Acc@5  81.25 ( 82.00)
Test: [101/156]	Time  0.025 ( 0.069)	Loss 1.9610e+00 (1.8886e+00)	Acc@1  25.00 ( 33.42)	Acc@5  81.25 ( 82.21)
Test: [106/156]	Time  0.025 ( 0.066)	Loss 2.0713e+00 (1.8893e+00)	Acc@1  21.88 ( 33.20)	Acc@5  65.62 ( 82.10)
Test: [111/156]	Time  0.025 ( 0.065)	Loss 2.1506e+00 (1.8907e+00)	Acc@1  34.38 ( 33.16)	Acc@5  62.50 ( 81.84)
Test: [116/156]	Time  0.025 ( 0.063)	Loss 2.0197e+00 (1.8922e+00)	Acc@1  31.25 ( 33.08)	Acc@5  78.12 ( 81.84)
Test: [121/156]	Time  0.025 ( 0.061)	Loss 2.0998e+00 (1.8938e+00)	Acc@1  28.12 ( 33.08)	Acc@5  84.38 ( 81.87)
Test: [126/156]	Time  0.025 ( 0.060)	Loss 1.7689e+00 (1.8907e+00)	Acc@1  34.38 ( 33.16)	Acc@5  87.50 ( 81.94)
Test: [131/156]	Time  0.025 ( 0.059)	Loss 1.8928e+00 (1.8874e+00)	Acc@1  40.62 ( 33.30)	Acc@5  81.25 ( 82.06)
Test: [136/156]	Time  0.025 ( 0.057)	Loss 1.8589e+00 (1.8863e+00)	Acc@1  34.38 ( 33.43)	Acc@5  93.75 ( 82.01)
Test: [141/156]	Time  0.025 ( 0.056)	Loss 1.8256e+00 (1.8911e+00)	Acc@1  31.25 ( 33.27)	Acc@5  87.50 ( 81.74)
Test: [146/156]	Time  0.026 ( 0.055)	Loss 2.0805e+00 (1.8888e+00)	Acc@1  21.88 ( 33.30)	Acc@5  75.00 ( 81.83)
Test: [151/156]	Time  0.025 ( 0.054)	Loss 1.7712e+00 (1.8873e+00)	Acc@1  34.38 ( 33.34)	Acc@5  81.25 ( 81.91)
Test: [156/156]	Time  0.024 ( 0.053)	Loss 1.9549e+00 (1.8903e+00)	Acc@1  25.00 ( 33.17)	Acc@5  87.50 ( 81.89)
 * Time 0.053 Loss 1.890 Acc@1 33.173 Acc@5 81.891
saving checkpoint at GPU:0 
 
Epoch: [4][ 1/39]	Time  4.443 ( 4.443)	Data  4.419 ( 4.419)	损失 2.0009e+00 (2.0009e+00)	准确率@1  31.25 ( 31.25)	准确率@5  81.25 ( 81.25)
Epoch: [4][ 6/39]	Time  0.025 ( 0.763)	Data  0.005 ( 0.742)	损失 1.8077e+00 (1.8928e+00)	准确率@1  40.62 ( 33.85)	准确率@5  90.62 ( 83.85)
Epoch: [4][11/39]	Time  0.026 ( 0.428)	Data  0.006 ( 0.407)	损失 2.1033e+00 (1.8979e+00)	准确率@1  25.00 ( 32.39)	准确率@5  71.88 ( 83.52)
Epoch: [4][16/39]	Time  0.025 ( 0.302)	Data  0.005 ( 0.282)	损失 1.8171e+00 (1.9205e+00)	准确率@1  31.25 ( 30.86)	准确率@5  87.50 ( 81.64)
Epoch: [4][21/39]	Time  0.023 ( 0.236)	Data  0.003 ( 0.216)	损失 1.9354e+00 (1.9048e+00)	准确率@1  34.38 ( 31.85)	准确率@5  78.12 ( 82.14)
Epoch: [4][26/39]	Time  0.026 ( 0.196)	Data  0.006 ( 0.176)	损失 1.9234e+00 (1.8976e+00)	准确率@1  31.25 ( 31.85)	准确率@5  84.38 ( 82.21)
Epoch: [4][31/39]	Time  0.025 ( 0.168)	Data  0.005 ( 0.148)	损失 1.7321e+00 (1.8724e+00)	准确率@1  43.75 ( 32.56)	准确率@5  90.62 ( 82.96)
Epoch: [4][36/39]	Time  0.027 ( 0.149)	Data  0.002 ( 0.128)	损失 1.7647e+00 (1.8606e+00)	准确率@1  43.75 ( 33.77)	准确率@5  84.38 ( 83.42)
Epoch: [4][ 1/39]	Time  4.438 ( 4.438)	Data  4.415 ( 4.415)	损失 1.8558e+00 (1.8558e+00)	准确率@1  46.88 ( 46.88)	准确率@5  81.25 ( 81.25)
Epoch: [4][ 6/39]	Time  0.026 ( 2.281)	Data  0.002 ( 2.258)	损失 1.8228e+00 (1.9020e+00)	准确率@1  43.75 ( 36.98)	准确率@5  84.38 ( 84.38)
Epoch: [4][11/39]	Time  0.026 ( 1.256)	Data  0.002 ( 1.233)	损失 1.8772e+00 (1.8999e+00)	准确率@1  25.00 ( 34.38)	准确率@5  84.38 ( 84.38)
Epoch: [4][16/39]	Time  0.026 ( 0.871)	Data  0.002 ( 0.849)	损失 2.0108e+00 (1.8942e+00)	准确率@1  34.38 ( 34.96)	准确率@5  78.12 ( 82.81)
Epoch: [4][21/39]	Time  0.030 ( 0.670)	Data  0.002 ( 0.647)	损失 1.8435e+00 (1.8692e+00)	准确率@1  46.88 ( 36.76)	准确率@5  75.00 ( 83.04)
Epoch: [4][26/39]	Time  0.025 ( 0.546)	Data  0.002 ( 0.523)	损失 1.7492e+00 (1.8543e+00)	准确率@1  34.38 ( 36.54)	准确率@5  84.38 ( 83.17)
Epoch: [4][31/39]	Time  0.027 ( 0.462)	Data  0.002 ( 0.440)	损失 1.8094e+00 (1.8514e+00)	准确率@1  31.25 ( 36.29)	准确率@5  84.38 ( 83.77)
Epoch: [4][36/39]	Time  0.027 ( 0.402)	Data  0.001 ( 0.379)	损失 1.6421e+00 (1.8390e+00)	准确率@1  37.50 ( 36.89)	准确率@5  93.75 ( 83.77)
Epoch: [4][ 1/39]	Time  4.548 ( 4.548)	Data  4.525 ( 4.525)	损失 1.8154e+00 (1.8154e+00)	准确率@1  31.25 ( 31.25)	准确率@5  93.75 ( 93.75)
Epoch: [4][ 6/39]	Time  0.026 ( 2.332)	Data  0.002 ( 2.310)	损失 1.7319e+00 (1.8292e+00)	准确率@1  46.88 ( 36.98)	准确率@5  84.38 ( 85.42)
Epoch: [4][11/39]	Time  0.026 ( 1.284)	Data  0.003 ( 1.261)	损失 1.9024e+00 (1.8376e+00)	准确率@1  28.12 ( 36.08)	准确率@5  84.38 ( 83.81)
Epoch: [4][16/39]	Time  0.026 ( 0.891)	Data  0.002 ( 0.868)	损失 1.6735e+00 (1.8514e+00)	准确率@1  43.75 ( 35.35)	准确率@5  90.62 ( 83.01)
Epoch: [4][21/39]	Time  0.030 ( 0.685)	Data  0.003 ( 0.662)	损失 2.0190e+00 (1.8568e+00)	准确率@1  31.25 ( 34.52)	准确率@5  81.25 ( 83.33)
Epoch: [4][26/39]	Time  0.027 ( 0.558)	Data  0.002 ( 0.536)	损失 1.9624e+00 (1.8456e+00)	准确率@1  28.12 ( 34.38)	准确率@5  81.25 ( 84.01)
Epoch: [4][31/39]	Time  0.028 ( 0.472)	Data  0.005 ( 0.450)	损失 1.9216e+00 (1.8393e+00)	准确率@1  25.00 ( 34.38)	准确率@5  84.38 ( 83.97)
Epoch: [4][36/39]	Time  0.027 ( 0.411)	Data  0.004 ( 0.388)	损失 1.7642e+00 (1.8280e+00)	准确率@1  37.50 ( 35.07)	准确率@5  90.62 ( 84.46)
Epoch: [4][ 1/39]	Time  4.602 ( 4.602)	Data  4.578 ( 4.578)	损失 1.8475e+00 (1.8475e+00)	准确率@1  31.25 ( 31.25)	准确率@5  81.25 ( 81.25)
Epoch: [4][ 6/39]	Time  0.026 ( 2.335)	Data  0.002 ( 2.312)	损失 1.9681e+00 (1.8320e+00)	准确率@1  18.75 ( 30.73)	准确率@5  75.00 ( 83.33)
Epoch: [4][11/39]	Time  0.026 ( 1.285)	Data  0.002 ( 1.263)	损失 1.9781e+00 (1.8736e+00)	准确率@1  25.00 ( 31.82)	准确率@5  75.00 ( 81.82)
Epoch: [4][16/39]	Time  0.026 ( 0.892)	Data  0.002 ( 0.869)	损失 2.0589e+00 (1.8609e+00)	准确率@1  28.12 ( 33.20)	准确率@5  75.00 ( 82.23)
Epoch: [4][21/39]	Time  0.030 ( 0.686)	Data  0.009 ( 0.663)	损失 1.7708e+00 (1.8612e+00)	准确率@1  46.88 ( 34.38)	准确率@5  81.25 ( 81.70)
Epoch: [4][26/39]	Time  0.025 ( 0.558)	Data  0.004 ( 0.537)	损失 1.6252e+00 (1.8336e+00)	准确率@1  56.25 ( 35.82)	准确率@5  93.75 ( 82.57)
Epoch: [4][31/39]	Time  0.028 ( 0.473)	Data  0.002 ( 0.451)	损失 1.9363e+00 (1.8201e+00)	准确率@1  37.50 ( 36.49)	准确率@5  87.50 ( 83.47)
Epoch: [4][36/39]	Time  0.028 ( 0.411)	Data  0.002 ( 0.389)	损失 1.8661e+00 (1.8042e+00)	准确率@1  28.12 ( 37.07)	准确率@5  84.38 ( 84.11)
Epoch: [4][ 1/39]	Time  4.572 ( 4.572)	Data  4.545 ( 4.545)	损失 1.9160e+00 (1.9160e+00)	准确率@1  31.25 ( 31.25)	准确率@5  81.25 ( 81.25)
Epoch: [4][ 6/39]	Time  0.026 ( 2.330)	Data  0.002 ( 2.306)	损失 1.8736e+00 (1.9054e+00)	准确率@1  34.38 ( 31.77)	准确率@5  93.75 ( 82.29)
Epoch: [4][11/39]	Time  0.026 ( 1.283)	Data  0.003 ( 1.259)	损失 1.8504e+00 (1.8636e+00)	准确率@1  37.50 ( 32.67)	准确率@5  84.38 ( 84.38)
Epoch: [4][16/39]	Time  0.026 ( 0.890)	Data  0.002 ( 0.867)	损失 1.9313e+00 (1.9013e+00)	准确率@1  28.12 ( 32.03)	准确率@5  65.62 ( 82.62)
Epoch: [4][21/39]	Time  0.030 ( 0.684)	Data  0.004 ( 0.661)	损失 1.7130e+00 (1.8820e+00)	准确率@1  28.12 ( 32.59)	准确率@5  96.88 ( 83.18)
Epoch: [4][26/39]	Time  0.025 ( 0.557)	Data  0.007 ( 0.535)	损失 1.7596e+00 (1.8624e+00)	准确率@1  34.38 ( 33.89)	准确率@5  87.50 ( 83.41)
Epoch: [4][31/39]	Time  0.026 ( 0.472)	Data  0.006 ( 0.450)	损失 1.7758e+00 (1.8449e+00)	准确率@1  40.62 ( 35.08)	准确率@5  81.25 ( 83.87)
Epoch: [4][36/39]	Time  0.028 ( 0.410)	Data  0.002 ( 0.388)	损失 1.7423e+00 (1.8329e+00)	准确率@1  46.88 ( 35.68)	准确率@5  78.12 ( 83.68)
Epoch: [4][ 1/39]	Time  4.660 ( 4.660)	Data  4.636 ( 4.636)	损失 1.9096e+00 (1.9096e+00)	准确率@1  25.00 ( 25.00)	准确率@5  81.25 ( 81.25)
Epoch: [4][ 6/39]	Time  0.026 ( 2.323)	Data  0.002 ( 2.300)	损失 1.5916e+00 (1.9024e+00)	准确率@1  43.75 ( 34.90)	准确率@5  90.62 ( 80.73)
Epoch: [4][11/39]	Time  0.026 ( 1.279)	Data  0.002 ( 1.256)	损失 1.8058e+00 (1.8886e+00)	准确率@1  28.12 ( 33.52)	准确率@5  87.50 ( 82.10)
Epoch: [4][16/39]	Time  0.026 ( 0.887)	Data  0.002 ( 0.865)	损失 2.1366e+00 (1.8770e+00)	准确率@1  28.12 ( 34.96)	准确率@5  71.88 ( 82.81)
Epoch: [4][21/39]	Time  0.029 ( 0.682)	Data  0.002 ( 0.659)	损失 1.9074e+00 (1.8562e+00)	准确率@1  40.62 ( 36.16)	准确率@5  75.00 ( 82.14)
Epoch: [4][26/39]	Time  0.025 ( 0.556)	Data  0.002 ( 0.533)	损失 1.9661e+00 (1.8536e+00)	准确率@1  28.12 ( 35.94)	准确率@5  81.25 ( 82.21)
Epoch: [4][31/39]	Time  0.027 ( 0.470)	Data  0.002 ( 0.448)	损失 1.8760e+00 (1.8624e+00)	准确率@1  34.38 ( 35.89)	准确率@5  78.12 ( 81.96)
Epoch: [4][36/39]	Time  0.027 ( 0.409)	Data  0.002 ( 0.386)	损失 1.6999e+00 (1.8540e+00)	准确率@1  31.25 ( 35.76)	准确率@5  90.62 ( 82.12)
Epoch: [4][ 1/39]	Time  4.563 ( 4.563)	Data  4.541 ( 4.541)	损失 2.0557e+00 (2.0557e+00)	准确率@1  25.00 ( 25.00)	准确率@5  65.62 ( 65.62)
Epoch: [4][ 6/39]	Time  0.027 ( 2.315)	Data  0.002 ( 2.292)	损失 1.7912e+00 (1.8816e+00)	准确率@1  28.12 ( 31.25)	准确率@5  90.62 ( 83.33)
Epoch: [4][11/39]	Time  0.026 ( 1.275)	Data  0.003 ( 1.252)	损失 1.7438e+00 (1.8211e+00)	准确率@1  43.75 ( 34.66)	准确率@5  90.62 ( 85.80)
Epoch: [4][16/39]	Time  0.026 ( 0.884)	Data  0.002 ( 0.862)	损失 1.7208e+00 (1.8229e+00)	准确率@1  37.50 ( 34.18)	准确率@5  84.38 ( 84.96)
Epoch: [4][21/39]	Time  0.031 ( 0.680)	Data  0.006 ( 0.658)	损失 1.8968e+00 (1.8415e+00)	准确率@1  31.25 ( 33.18)	准确率@5  81.25 ( 84.08)
Epoch: [4][26/39]	Time  0.026 ( 0.554)	Data  0.002 ( 0.532)	损失 2.0969e+00 (1.8680e+00)	准确率@1  18.75 ( 32.21)	准确率@5  81.25 ( 83.89)
Epoch: [4][31/39]	Time  0.027 ( 0.469)	Data  0.002 ( 0.446)	损失 1.9609e+00 (1.8646e+00)	准确率@1  28.12 ( 32.16)	准确率@5  87.50 ( 84.07)
Epoch: [4][36/39]	Time  0.027 ( 0.407)	Data  0.002 ( 0.385)	损失 1.8032e+00 (1.8555e+00)	准确率@1  37.50 ( 32.64)	准确率@5  78.12 ( 84.03)
Epoch: [4][ 1/39]	Time  4.490 ( 4.490)	Data  4.462 ( 4.462)	损失 1.9383e+00 (1.9383e+00)	准确率@1  34.38 ( 34.38)	准确率@5  84.38 ( 84.38)
Epoch: [4][ 6/39]	Time  0.027 ( 2.316)	Data  0.002 ( 2.292)	损失 1.8910e+00 (1.9066e+00)	准确率@1  37.50 ( 32.29)	准确率@5  78.12 ( 79.69)
Epoch: [4][11/39]	Time  0.026 ( 1.275)	Data  0.003 ( 1.252)	损失 1.7293e+00 (1.8645e+00)	准确率@1  34.38 ( 33.52)	准确率@5  90.62 ( 82.95)
Epoch: [4][16/39]	Time  0.026 ( 0.884)	Data  0.002 ( 0.862)	损失 1.7126e+00 (1.8764e+00)	准确率@1  43.75 ( 32.42)	准确率@5  93.75 ( 83.59)
Epoch: [4][21/39]	Time  0.031 ( 0.680)	Data  0.003 ( 0.657)	损失 1.8019e+00 (1.9020e+00)	准确率@1  34.38 ( 31.70)	准确率@5  81.25 ( 82.29)
Epoch: [4][26/39]	Time  0.026 ( 0.554)	Data  0.002 ( 0.531)	损失 1.8513e+00 (1.8881e+00)	准确率@1  37.50 ( 32.69)	准确率@5  81.25 ( 82.57)
Epoch: [4][31/39]	Time  0.026 ( 0.469)	Data  0.003 ( 0.447)	损失 1.9034e+00 (1.8681e+00)	准确率@1  25.00 ( 33.17)	准确率@5  84.38 ( 83.57)
Epoch: [4][36/39]	Time  0.027 ( 0.408)	Data  0.002 ( 0.385)	损失 1.6557e+00 (1.8449e+00)	准确率@1  37.50 ( 33.42)	准确率@5  90.62 ( 84.90)
Test: [  1/156]	Time  4.561 ( 4.561)	Loss 1.6298e+00 (1.6298e+00)	Acc@1  34.38 ( 34.38)	Acc@5  87.50 ( 87.50)
Test: [  6/156]	Time  0.023 ( 0.779)	Loss 1.9472e+00 (1.7287e+00)	Acc@1  34.38 ( 40.10)	Acc@5  84.38 ( 87.50)
Test: [ 11/156]	Time  0.024 ( 0.436)	Loss 1.4856e+00 (1.6882e+00)	Acc@1  46.88 ( 41.19)	Acc@5 100.00 ( 88.64)
Test: [ 16/156]	Time  0.024 ( 0.307)	Loss 1.6506e+00 (1.6892e+00)	Acc@1  40.62 ( 41.21)	Acc@5  81.25 ( 87.11)
Test: [ 21/156]	Time  0.023 ( 0.239)	Loss 1.6568e+00 (1.7084e+00)	Acc@1  43.75 ( 40.03)	Acc@5  87.50 ( 86.46)
Test: [ 26/156]	Time  0.024 ( 0.198)	Loss 1.7715e+00 (1.7591e+00)	Acc@1  37.50 ( 37.86)	Acc@5  93.75 ( 86.42)
Test: [ 31/156]	Time  0.025 ( 0.170)	Loss 1.5738e+00 (1.7595e+00)	Acc@1  53.12 ( 38.51)	Acc@5  90.62 ( 86.29)
Test: [ 36/156]	Time  0.024 ( 0.150)	Loss 1.8858e+00 (1.7648e+00)	Acc@1  21.88 ( 38.54)	Acc@5  87.50 ( 86.20)
Test: [ 41/156]	Time  0.024 ( 0.135)	Loss 1.6293e+00 (1.7681e+00)	Acc@1  53.12 ( 39.02)	Acc@5  87.50 ( 85.59)
Test: [ 46/156]	Time  0.025 ( 0.123)	Loss 1.7998e+00 (1.7693e+00)	Acc@1  43.75 ( 39.40)	Acc@5  90.62 ( 85.80)
Test: [ 51/156]	Time  0.025 ( 0.113)	Loss 1.6828e+00 (1.7643e+00)	Acc@1  43.75 ( 39.89)	Acc@5  93.75 ( 86.09)
Test: [ 56/156]	Time  0.025 ( 0.105)	Loss 1.7178e+00 (1.7656e+00)	Acc@1  34.38 ( 39.51)	Acc@5  84.38 ( 85.99)
Test: [ 61/156]	Time  0.024 ( 0.099)	Loss 2.0675e+00 (1.7788e+00)	Acc@1  21.88 ( 39.34)	Acc@5  84.38 ( 85.35)
Test: [ 66/156]	Time  0.025 ( 0.093)	Loss 1.6466e+00 (1.7720e+00)	Acc@1  46.88 ( 39.44)	Acc@5  84.38 ( 85.75)
Test: [ 71/156]	Time  0.025 ( 0.088)	Loss 1.6512e+00 (1.7722e+00)	Acc@1  37.50 ( 39.04)	Acc@5  87.50 ( 86.09)
Test: [ 76/156]	Time  0.025 ( 0.084)	Loss 1.7300e+00 (1.7724e+00)	Acc@1  25.00 ( 38.57)	Acc@5  81.25 ( 86.10)
Test: [ 81/156]	Time  0.025 ( 0.080)	Loss 1.6290e+00 (1.7737e+00)	Acc@1  53.12 ( 38.58)	Acc@5  96.88 ( 86.15)
Test: [ 86/156]	Time  0.024 ( 0.077)	Loss 2.1274e+00 (1.7738e+00)	Acc@1  21.88 ( 38.41)	Acc@5  75.00 ( 86.12)
Test: [ 91/156]	Time  0.025 ( 0.074)	Loss 1.9039e+00 (1.7732e+00)	Acc@1  31.25 ( 38.50)	Acc@5  75.00 ( 86.13)
Test: [ 96/156]	Time  0.024 ( 0.072)	Loss 1.7013e+00 (1.7699e+00)	Acc@1  37.50 ( 38.64)	Acc@5  93.75 ( 86.10)
Test: [101/156]	Time  0.025 ( 0.069)	Loss 1.7992e+00 (1.7669e+00)	Acc@1  31.25 ( 38.80)	Acc@5  93.75 ( 86.23)
Test: [106/156]	Time  0.026 ( 0.067)	Loss 1.6680e+00 (1.7673e+00)	Acc@1  40.62 ( 38.50)	Acc@5  90.62 ( 86.26)
Test: [111/156]	Time  0.026 ( 0.065)	Loss 2.1875e+00 (1.7723e+00)	Acc@1  25.00 ( 38.40)	Acc@5  68.75 ( 86.06)
Test: [116/156]	Time  0.026 ( 0.064)	Loss 1.7385e+00 (1.7741e+00)	Acc@1  37.50 ( 38.25)	Acc@5  87.50 ( 86.07)
Test: [121/156]	Time  0.025 ( 0.062)	Loss 1.7443e+00 (1.7764e+00)	Acc@1  34.38 ( 38.17)	Acc@5  84.38 ( 85.85)
Test: [126/156]	Time  0.026 ( 0.061)	Loss 1.5775e+00 (1.7750e+00)	Acc@1  59.38 ( 38.24)	Acc@5  87.50 ( 85.84)
Test: [131/156]	Time  0.026 ( 0.059)	Loss 1.8699e+00 (1.7738e+00)	Acc@1  31.25 ( 38.24)	Acc@5  84.38 ( 85.90)
Test: [136/156]	Time  0.026 ( 0.058)	Loss 1.6970e+00 (1.7716e+00)	Acc@1  37.50 ( 38.26)	Acc@5  93.75 ( 86.10)
Test: [141/156]	Time  0.025 ( 0.057)	Loss 1.7991e+00 (1.7728e+00)	Acc@1  18.75 ( 37.94)	Acc@5  87.50 ( 86.06)
Test: [146/156]	Time  0.025 ( 0.056)	Loss 1.9463e+00 (1.7722e+00)	Acc@1  25.00 ( 37.76)	Acc@5  90.62 ( 86.09)
Test: [151/156]	Time  0.025 ( 0.055)	Loss 1.6180e+00 (1.7713e+00)	Acc@1  43.75 ( 37.85)	Acc@5  87.50 ( 86.03)
Test: [156/156]	Time  0.024 ( 0.054)	Loss 1.9039e+00 (1.7728e+00)	Acc@1  18.75 ( 37.62)	Acc@5  90.62 ( 86.04)
 * Time 0.054 Loss 1.773 Acc@1 37.620 Acc@5 86.038
saving checkpoint at GPU:0 
 
Fianl validating at NPU:0
Test: [  1/156]	Time  4.485 ( 4.485)	Loss 1.6370e+00 (1.6370e+00)	Acc@1  40.62 ( 40.62)	Acc@5  87.50 ( 87.50)
Test: [  6/156]	Time  0.024 ( 0.767)	Loss 1.7751e+00 (1.7937e+00)	Acc@1  40.62 ( 40.10)	Acc@5  81.25 ( 84.90)
Test: [ 11/156]	Time  0.024 ( 0.429)	Loss 1.4796e+00 (1.7579e+00)	Acc@1  46.88 ( 39.49)	Acc@5  96.88 ( 86.08)
Test: [ 16/156]	Time  0.024 ( 0.303)	Loss 1.7212e+00 (1.7654e+00)	Acc@1  40.62 ( 38.09)	Acc@5  90.62 ( 86.33)
Test: [ 21/156]	Time  0.024 ( 0.236)	Loss 1.9206e+00 (1.7974e+00)	Acc@1  37.50 ( 37.95)	Acc@5  84.38 ( 85.71)
Test: [ 26/156]	Time  0.024 ( 0.195)	Loss 1.6012e+00 (1.8080e+00)	Acc@1  56.25 ( 37.98)	Acc@5  90.62 ( 84.98)
Test: [ 31/156]	Time  0.024 ( 0.168)	Loss 1.8804e+00 (1.8058e+00)	Acc@1  37.50 ( 37.50)	Acc@5  87.50 ( 84.98)
Test: [ 36/156]	Time  0.024 ( 0.148)	Loss 1.8839e+00 (1.8068e+00)	Acc@1  34.38 ( 37.33)	Acc@5  87.50 ( 85.33)
Test: [ 41/156]	Time  0.024 ( 0.133)	Loss 1.7831e+00 (1.8124e+00)	Acc@1  37.50 ( 37.20)	Acc@5  90.62 ( 85.37)
Test: [ 46/156]	Time  0.024 ( 0.121)	Loss 1.7838e+00 (1.8037e+00)	Acc@1  28.12 ( 37.43)	Acc@5  90.62 ( 85.80)
Test: [ 51/156]	Time  0.024 ( 0.111)	Loss 1.8011e+00 (1.7922e+00)	Acc@1  40.62 ( 37.75)	Acc@5  87.50 ( 86.15)
Test: [ 56/156]	Time  0.024 ( 0.103)	Loss 1.5840e+00 (1.7934e+00)	Acc@1  53.12 ( 37.50)	Acc@5  87.50 ( 86.38)
Test: [ 61/156]	Time  0.024 ( 0.097)	Loss 1.9514e+00 (1.8033e+00)	Acc@1  21.88 ( 36.94)	Acc@5  87.50 ( 86.17)
Test: [ 66/156]	Time  0.024 ( 0.091)	Loss 1.6100e+00 (1.7868e+00)	Acc@1  34.38 ( 37.50)	Acc@5  93.75 ( 86.55)
Test: [ 71/156]	Time  0.024 ( 0.087)	Loss 1.8780e+00 (1.7804e+00)	Acc@1  43.75 ( 38.03)	Acc@5  68.75 ( 86.53)
Test: [ 76/156]	Time  0.025 ( 0.083)	Loss 1.7227e+00 (1.7797e+00)	Acc@1  37.50 ( 37.91)	Acc@5  81.25 ( 86.43)
Test: [ 81/156]	Time  0.025 ( 0.079)	Loss 1.9444e+00 (1.7870e+00)	Acc@1  28.12 ( 37.62)	Acc@5  81.25 ( 86.15)
Test: [ 86/156]	Time  0.027 ( 0.076)	Loss 1.8905e+00 (1.7831e+00)	Acc@1  34.38 ( 37.68)	Acc@5  75.00 ( 86.16)
Test: [ 91/156]	Time  0.025 ( 0.073)	Loss 1.9310e+00 (1.7840e+00)	Acc@1  37.50 ( 37.50)	Acc@5  81.25 ( 86.26)
Test: [ 96/156]	Time  0.025 ( 0.071)	Loss 1.6597e+00 (1.7792e+00)	Acc@1  56.25 ( 37.83)	Acc@5  87.50 ( 86.46)
Test: [101/156]	Time  0.025 ( 0.068)	Loss 1.8971e+00 (1.7789e+00)	Acc@1  28.12 ( 37.78)	Acc@5  90.62 ( 86.57)
Test: [106/156]	Time  0.025 ( 0.066)	Loss 1.8971e+00 (1.7837e+00)	Acc@1  37.50 ( 37.59)	Acc@5  81.25 ( 86.41)
Test: [111/156]	Time  0.025 ( 0.064)	Loss 1.7543e+00 (1.7870e+00)	Acc@1  31.25 ( 37.25)	Acc@5  78.12 ( 86.12)
Test: [116/156]	Time  0.025 ( 0.063)	Loss 1.9240e+00 (1.7878e+00)	Acc@1  25.00 ( 37.18)	Acc@5  81.25 ( 86.02)
Test: [121/156]	Time  0.025 ( 0.061)	Loss 1.6417e+00 (1.7872e+00)	Acc@1  40.62 ( 37.04)	Acc@5  93.75 ( 85.95)
Test: [126/156]	Time  0.026 ( 0.060)	Loss 1.6461e+00 (1.7860e+00)	Acc@1  46.88 ( 37.05)	Acc@5  84.38 ( 85.94)
Test: [131/156]	Time  0.029 ( 0.059)	Loss 1.7743e+00 (1.7820e+00)	Acc@1  37.50 ( 37.17)	Acc@5  84.38 ( 85.90)
Test: [136/156]	Time  0.025 ( 0.057)	Loss 1.6772e+00 (1.7801e+00)	Acc@1  37.50 ( 37.06)	Acc@5  90.62 ( 86.05)
Test: [141/156]	Time  0.025 ( 0.056)	Loss 1.5843e+00 (1.7800e+00)	Acc@1  34.38 ( 36.99)	Acc@5 100.00 ( 86.26)
Test: [146/156]	Time  0.027 ( 0.055)	Loss 1.9931e+00 (1.7807e+00)	Acc@1  28.12 ( 36.94)	Acc@5  93.75 ( 86.26)
Test: [151/156]	Time  0.025 ( 0.054)	Loss 1.5271e+00 (1.7776e+00)	Acc@1  53.12 ( 37.04)	Acc@5  96.88 ( 86.30)
Test: [156/156]	Time  0.024 ( 0.053)	Loss 1.6567e+00 (1.7783e+00)	Acc@1  34.38 ( 36.96)	Acc@5  93.75 ( 86.28)
 * Time 0.053 Loss 1.778 Acc@1 36.959 Acc@5 86.278
total time:266.3671946525574
